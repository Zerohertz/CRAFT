v0.0.1
%319 = fn (%v0:0: Tensor[(1, 3, 2400, 2400), float32]) -> (Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32]) {
  %0 = transpose(%v0:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 3), float32]
  %1 = nn.pad(%0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 3), float32]
  %2 = transpose(%1, framework_op_name="transpose6", output_tensors_name=["transpose6:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 3, 2402, 2402), float32]
  %3 = nn.pad(%2, framework_op_name="nn.pad0", output_tensors_name=["nn.pad0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", pad_width=[[0, 0], [0, 0], [0, 0], [0, 2]]) // ty=Tensor[(1, 3, 2402, 2404), float32]
  %4 = nn.conv2d(%3, meta[relay.Constant][0] // ty=Tensor[(3, 3, 3, 64), float32], framework_op_name="nn.conv2d0", output_tensors_name=["nn.conv2d0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], kernel_layout="HWIO") // ty=Tensor[(1, 64, 2400, 2402), float32]
  %5 = strided_slice(%4, framework_op_name="strided_slice0", output_tensors_name=["strided_slice0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", begin=[0, 0, 0, 0], end=[1, 64, 2400, 2400], strides=[1, 1, 1, 1]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %6 = transpose(%5, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %7 = add(%6, meta[relay.Constant][1] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %8 = transpose(%7, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %9 = transpose(%8, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %10 = subtract(%9, meta[relay.Constant][2] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %11 = multiply(%10, meta[relay.Constant][3] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %12 = multiply(%11, meta[relay.Constant][4] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %13 = add(%12, meta[relay.Constant][5] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %14 = transpose(%13, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %15 = nn.relu(%14, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %16 = transpose(%15, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %17 = nn.pad(%16, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 64), float32]
  %18 = nn.conv2d(%17, meta[relay.Constant][6] // ty=Tensor[(3, 3, 64, 64), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 2400, 2400, 64), float32]
  %19 = add(%18, meta[relay.Constant][7] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %20 = transpose(%19, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %21 = transpose(%20, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %22 = subtract(%21, meta[relay.Constant][8] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %23 = multiply(%22, meta[relay.Constant][9] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %24 = multiply(%23, meta[relay.Constant][10] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %25 = add(%24, meta[relay.Constant][11] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %26 = transpose(%25, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %27 = nn.relu(%26, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %28 = transpose(%27, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %29 = nn.max_pool2d(%28, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 1200, 1200, 64), float32]
  %30 = transpose(%29, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %31 = transpose(%30, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %32 = nn.pad(%31, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
  %33 = nn.conv2d(%32, meta[relay.Constant][12] // ty=Tensor[(3, 3, 64, 128), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
  %34 = add(%33, meta[relay.Constant][13] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %35 = transpose(%34, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %36 = transpose(%35, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %37 = subtract(%36, meta[relay.Constant][14] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %38 = multiply(%37, meta[relay.Constant][15] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %39 = multiply(%38, meta[relay.Constant][16] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %40 = add(%39, meta[relay.Constant][17] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %41 = transpose(%40, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %42 = nn.relu(%41, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %43 = transpose(%42, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %44 = nn.pad(%43, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 128), float32]
  %45 = nn.conv2d(%44, meta[relay.Constant][18] // ty=Tensor[(3, 3, 128, 128), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
  %46 = add(%45, meta[relay.Constant][19] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %47 = transpose(%46, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %48 = transpose(%47, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %49 = subtract(%48, meta[relay.Constant][20] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %50 = multiply(%49, meta[relay.Constant][21] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %51 = multiply(%50, meta[relay.Constant][22] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %52 = add(%51, meta[relay.Constant][23] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %53 = transpose(%52, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %54 = nn.relu(%53, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %55 = transpose(%54, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %56 = nn.max_pool2d(%55, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 600, 600, 128), float32]
  %57 = transpose(%56, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %58 = transpose(%57, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %59 = nn.pad(%58, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
  %60 = nn.conv2d(%59, meta[relay.Constant][24] // ty=Tensor[(3, 3, 128, 256), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
  %61 = add(%60, meta[relay.Constant][25] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %62 = transpose(%61, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %63 = transpose(%62, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %64 = subtract(%63, meta[relay.Constant][26] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %65 = multiply(%64, meta[relay.Constant][27] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %66 = multiply(%65, meta[relay.Constant][28] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %67 = add(%66, meta[relay.Constant][29] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %68 = transpose(%67, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %69 = nn.relu(%68, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
  %70 = transpose(%69, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %71 = nn.pad(%70, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
  %72 = nn.conv2d(%71, meta[relay.Constant][30] // ty=Tensor[(3, 3, 256, 256), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
  %73 = add(%72, meta[relay.Constant][31] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %74 = transpose(%73, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %75 = transpose(%74, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %76 = subtract(%75, meta[relay.Constant][32] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %77 = multiply(%76, meta[relay.Constant][33] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %78 = multiply(%77, meta[relay.Constant][34] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %79 = add(%78, meta[relay.Constant][35] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %80 = transpose(%79, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %81 = nn.relu(%80, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
  %82 = transpose(%81, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %83 = nn.pad(%82, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
  %84 = nn.conv2d(%83, meta[relay.Constant][36] // ty=Tensor[(3, 3, 256, 256), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
  %85 = add(%84, meta[relay.Constant][37] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %86 = transpose(%85, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %87 = transpose(%86, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %88 = subtract(%87, meta[relay.Constant][38] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %89 = multiply(%88, meta[relay.Constant][39] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %90 = multiply(%89, meta[relay.Constant][40] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %91 = add(%90, meta[relay.Constant][41] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %92 = transpose(%91, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %93 = nn.relu(%92, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
  %94 = transpose(%93, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %95 = nn.max_pool2d(%94, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 300, 300, 256), float32]
  %96 = transpose(%95, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %97 = transpose(%96, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %98 = nn.pad(%97, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
  %99 = nn.conv2d(%98, meta[relay.Constant][42] // ty=Tensor[(3, 3, 256, 512), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
  %100 = add(%99, meta[relay.Constant][43] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %101 = transpose(%100, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %102 = transpose(%101, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %103 = subtract(%102, meta[relay.Constant][44] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %104 = multiply(%103, meta[relay.Constant][45] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %105 = multiply(%104, meta[relay.Constant][46] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %106 = add(%105, meta[relay.Constant][47] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %107 = transpose(%106, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %108 = nn.relu(%107, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
  %109 = transpose(%108, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %110 = nn.pad(%109, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
  %111 = nn.conv2d(%110, meta[relay.Constant][48] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
  %112 = add(%111, meta[relay.Constant][49] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %113 = transpose(%112, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %114 = transpose(%113, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %115 = subtract(%114, meta[relay.Constant][50] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %116 = multiply(%115, meta[relay.Constant][51] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %117 = multiply(%116, meta[relay.Constant][52] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %118 = add(%117, meta[relay.Constant][53] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %119 = transpose(%118, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %120 = nn.relu(%119, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
  %121 = transpose(%120, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %122 = nn.pad(%121, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
  %123 = nn.conv2d(%122, meta[relay.Constant][54] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
  %124 = add(%123, meta[relay.Constant][55] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %125 = transpose(%124, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %126 = transpose(%125, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %127 = subtract(%126, meta[relay.Constant][56] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %128 = multiply(%127, meta[relay.Constant][57] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %129 = multiply(%128, meta[relay.Constant][58] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %130 = add(%129, meta[relay.Constant][59] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %131 = transpose(%130, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %132 = nn.relu(%131, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
  %133 = transpose(%132, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %134 = nn.max_pool2d(%133, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
  %135 = transpose(%134, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %136 = transpose(%135, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %137 = nn.pad(%136, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %138 = nn.conv2d(%137, meta[relay.Constant][60] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
  %139 = add(%138, meta[relay.Constant][61] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %140 = transpose(%139, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %141 = transpose(%140, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %142 = subtract(%141, meta[relay.Constant][62] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %143 = multiply(%142, meta[relay.Constant][63] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %144 = multiply(%143, meta[relay.Constant][64] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %145 = add(%144, meta[relay.Constant][65] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %146 = transpose(%145, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %147 = nn.relu(%146, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
  %148 = transpose(%147, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %149 = nn.pad(%148, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %150 = nn.conv2d(%149, meta[relay.Constant][66] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
  %151 = add(%150, meta[relay.Constant][67] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %152 = transpose(%151, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %153 = transpose(%152, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %154 = subtract(%153, meta[relay.Constant][68] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %155 = multiply(%154, meta[relay.Constant][69] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %156 = multiply(%155, meta[relay.Constant][70] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %157 = add(%156, meta[relay.Constant][71] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %158 = transpose(%157, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %159 = transpose(%158, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %160 = nn.pad(%159, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", pad_value=-65000, pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %161 = nn.max_pool2d(%160, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", pool_size=[3, 3], pool_type="", layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
  %162 = transpose(%161, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %163 = transpose(%162, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %164 = nn.pad(%163, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", pad_width=[[0, 0], [6, 6], [6, 6], [0, 0]]) // ty=Tensor[(1, 162, 162, 512), float32]
  %165 = nn.conv2d(%164, meta[relay.Constant][72] // ty=Tensor[(3, 3, 512, 1024), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", dilation=[6, 6], channels=1024, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
  %166 = add(%165, meta[relay.Constant][73] // ty=Tensor[(1024,), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
  %167 = transpose(%166, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
  %168 = transpose(%167, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1024), float32]
  %169 = nn.conv2d(%168, meta[relay.Constant][74] // ty=Tensor[(1, 1, 1024, 1024), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", channels=1024, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
  %170 = add(%169, meta[relay.Constant][75] // ty=Tensor[(1024,), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
  %171 = transpose(%170, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
  %172 = (%171, %158)
  %173 = concatenate(%172, framework_op_name="aten_cat/concat", output_tensors_name=["aten_cat/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat/concat", axis=1) // ty=Tensor[(1, 1536, 150, 150), float32]
  %174 = transpose(%173, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1536), float32]
  %175 = nn.conv2d(%174, meta[relay.Constant][76] // ty=Tensor[(1, 1, 1536, 512), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=512, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
  %176 = add(%175, meta[relay.Constant][77] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %177 = transpose(%176, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %178 = transpose(%177, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %179 = subtract(%178, meta[relay.Constant][78] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %180 = multiply(%179, meta[relay.Constant][79] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %181 = multiply(%180, meta[relay.Constant][80] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %182 = add(%181, meta[relay.Constant][81] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %183 = transpose(%182, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %184 = nn.relu(%183, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
  %185 = transpose(%184, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %186 = nn.pad(%185, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %187 = nn.conv2d(%186, meta[relay.Constant][82] // ty=Tensor[(3, 3, 512, 256), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 256), float32]
  %188 = add(%187, meta[relay.Constant][83] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %189 = transpose(%188, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
  %190 = transpose(%189, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
  %191 = subtract(%190, meta[relay.Constant][84] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %192 = multiply(%191, meta[relay.Constant][85] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %193 = multiply(%192, meta[relay.Constant][86] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %194 = add(%193, meta[relay.Constant][87] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %195 = transpose(%194, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
  %196 = nn.relu(%195, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 150, 150), float32]
  %197 = transpose(%196, framework_op_name="aten_upsample_bilinear2d/transpose", output_tensors_name=["aten_upsample_bilinear2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
  %198 = transpose(%197, framework_op_name="transpose0", output_tensors_name=["transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
  %199 = nn.conv2d_transpose(%198, meta[relay.Constant][88] // ty=Tensor[(256, 256, 4, 4), float32], framework_op_name="nn.conv2d_transpose0", output_tensors_name=["nn.conv2d_transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", channels=256, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 256, 300, 300), float32]
  %200 = transpose(%199, framework_op_name="aten_upsample_bilinear2d/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %201 = transpose(%200, framework_op_name="aten_upsample_bilinear2d/transpose_1", output_tensors_name=["aten_upsample_bilinear2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %202 = multiply(%201, meta[relay.Constant][89] // ty=Tensor[(300, 300), float32], framework_op_name="aten_upsample_bilinear2d/Mul", output_tensors_name=["aten_upsample_bilinear2d/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/Mul", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
  %203 = (%202, %120)
  %204 = concatenate(%203, framework_op_name="aten_cat_1/concat", output_tensors_name=["aten_cat_1/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_1/concat", axis=1) // ty=Tensor[(1, 768, 300, 300), float32]
  %205 = transpose(%204, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 768), float32]
  %206 = nn.conv2d(%205, meta[relay.Constant][90] // ty=Tensor[(1, 1, 768, 256), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=256, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 256), float32]
  %207 = add(%206, meta[relay.Constant][91] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %208 = transpose(%207, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %209 = transpose(%208, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %210 = subtract(%209, meta[relay.Constant][92] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %211 = multiply(%210, meta[relay.Constant][93] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %212 = multiply(%211, meta[relay.Constant][94] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %213 = add(%212, meta[relay.Constant][95] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %214 = transpose(%213, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %215 = nn.relu(%214, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
  %216 = transpose(%215, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %217 = nn.pad(%216, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
  %218 = nn.conv2d(%217, meta[relay.Constant][96] // ty=Tensor[(3, 3, 256, 128), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 128), float32]
  %219 = add(%218, meta[relay.Constant][97] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %220 = transpose(%219, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
  %221 = transpose(%220, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
  %222 = subtract(%221, meta[relay.Constant][98] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %223 = multiply(%222, meta[relay.Constant][99] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %224 = multiply(%223, meta[relay.Constant][100] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %225 = add(%224, meta[relay.Constant][101] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %226 = transpose(%225, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
  %227 = nn.relu(%226, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 300, 300), float32]
  %228 = transpose(%227, framework_op_name="aten_upsample_bilinear2d_1/transpose", output_tensors_name=["aten_upsample_bilinear2d_1/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
  %229 = transpose(%228, framework_op_name="transpose2", output_tensors_name=["transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
  %230 = nn.conv2d_transpose(%229, meta[relay.Constant][102] // ty=Tensor[(128, 128, 4, 4), float32], framework_op_name="nn.conv2d_transpose1", output_tensors_name=["nn.conv2d_transpose1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", channels=128, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 128, 600, 600), float32]
  %231 = transpose(%230, framework_op_name="aten_upsample_bilinear2d_1/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_1/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %232 = transpose(%231, framework_op_name="aten_upsample_bilinear2d_1/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_1/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %233 = multiply(%232, meta[relay.Constant][103] // ty=Tensor[(600, 600), float32], framework_op_name="aten_upsample_bilinear2d_1/Mul", output_tensors_name=["aten_upsample_bilinear2d_1/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/Mul", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
  %234 = (%233, %81)
  %235 = concatenate(%234, framework_op_name="aten_cat_2/concat", output_tensors_name=["aten_cat_2/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_2/concat", axis=1) // ty=Tensor[(1, 384, 600, 600), float32]
  %236 = transpose(%235, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 384), float32]
  %237 = nn.conv2d(%236, meta[relay.Constant][104] // ty=Tensor[(1, 1, 384, 128), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=128, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 128), float32]
  %238 = add(%237, meta[relay.Constant][105] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %239 = transpose(%238, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %240 = transpose(%239, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %241 = subtract(%240, meta[relay.Constant][106] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %242 = multiply(%241, meta[relay.Constant][107] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %243 = multiply(%242, meta[relay.Constant][108] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %244 = add(%243, meta[relay.Constant][109] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %245 = transpose(%244, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %246 = nn.relu(%245, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
  %247 = transpose(%246, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %248 = nn.pad(%247, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
  %249 = nn.conv2d(%248, meta[relay.Constant][110] // ty=Tensor[(3, 3, 128, 64), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 64), float32]
  %250 = add(%249, meta[relay.Constant][111] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %251 = transpose(%250, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
  %252 = transpose(%251, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
  %253 = subtract(%252, meta[relay.Constant][112] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %254 = multiply(%253, meta[relay.Constant][113] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %255 = multiply(%254, meta[relay.Constant][114] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %256 = add(%255, meta[relay.Constant][115] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %257 = transpose(%256, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
  %258 = nn.relu(%257, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 600, 600), float32]
  %259 = transpose(%258, framework_op_name="aten_upsample_bilinear2d_2/transpose", output_tensors_name=["aten_upsample_bilinear2d_2/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
  %260 = transpose(%259, framework_op_name="transpose4", output_tensors_name=["transpose4:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
  %261 = nn.conv2d_transpose(%260, meta[relay.Constant][116] // ty=Tensor[(64, 64, 4, 4), float32], framework_op_name="nn.conv2d_transpose2", output_tensors_name=["nn.conv2d_transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", channels=64, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %262 = transpose(%261, framework_op_name="aten_upsample_bilinear2d_2/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_2/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %263 = transpose(%262, framework_op_name="aten_upsample_bilinear2d_2/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_2/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %264 = multiply(%263, meta[relay.Constant][117] // ty=Tensor[(1200, 1200), float32], framework_op_name="aten_upsample_bilinear2d_2/Mul", output_tensors_name=["aten_upsample_bilinear2d_2/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/Mul", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %265 = (%264, %54)
  %266 = concatenate(%265, framework_op_name="aten_cat_3/concat", output_tensors_name=["aten_cat_3/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_3/concat", axis=1) // ty=Tensor[(1, 192, 1200, 1200), float32]
  %267 = transpose(%266, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 192), float32]
  %268 = nn.conv2d(%267, meta[relay.Constant][118] // ty=Tensor[(1, 1, 192, 64), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=64, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 64), float32]
  %269 = add(%268, meta[relay.Constant][119] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %270 = transpose(%269, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %271 = transpose(%270, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %272 = subtract(%271, meta[relay.Constant][120] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %273 = multiply(%272, meta[relay.Constant][121] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %274 = multiply(%273, meta[relay.Constant][122] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %275 = add(%274, meta[relay.Constant][123] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %276 = transpose(%275, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %277 = nn.relu(%276, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %278 = transpose(%277, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %279 = nn.pad(%278, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
  %280 = nn.conv2d(%279, meta[relay.Constant][124] // ty=Tensor[(3, 3, 64, 32), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
  %281 = add(%280, meta[relay.Constant][125] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %282 = transpose(%281, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %283 = transpose(%282, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %284 = subtract(%283, meta[relay.Constant][126] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %285 = multiply(%284, meta[relay.Constant][127] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %286 = multiply(%285, meta[relay.Constant][128] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %287 = add(%286, meta[relay.Constant][129] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %288 = transpose(%287, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %289 = nn.relu(%288, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %290 = transpose(%289, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %291 = nn.pad(%290, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
  %292 = nn.conv2d(%291, meta[relay.Constant][130] // ty=Tensor[(3, 3, 32, 32), float32], framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
  %293 = add(%292, meta[relay.Constant][131] // ty=Tensor[(32,), float32], framework_op_name="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %294 = transpose(%293, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %295 = nn.relu(%294, framework_op_name="Sequential_60/Conv2d_9/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_9/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %296 = transpose(%295, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %297 = nn.pad(%296, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
  %298 = nn.conv2d(%297, meta[relay.Constant][132] // ty=Tensor[(3, 3, 32, 32), float32], framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
  %299 = add(%298, meta[relay.Constant][133] // ty=Tensor[(32,), float32], framework_op_name="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %300 = transpose(%299, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %301 = nn.relu(%300, framework_op_name="Sequential_60/Conv2d_11/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_11/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %302 = transpose(%301, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %303 = nn.pad(%302, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
  %304 = nn.conv2d(%303, meta[relay.Constant][134] // ty=Tensor[(3, 3, 32, 16), float32], framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Conv2D", channels=16, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
  %305 = add(%304, meta[relay.Constant][135] // ty=Tensor[(16,), float32], framework_op_name="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %306 = transpose(%305, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %307 = nn.relu(%306, framework_op_name="Sequential_60/Conv2d_13/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %308 = transpose(%307, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %309 = nn.conv2d(%308, meta[relay.Constant][136] // ty=Tensor[(1, 1, 16, 16), float32], framework_op_name="Sequential_60/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/Conv2D", channels=16, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
  %310 = add(%309, meta[relay.Constant][137] // ty=Tensor[(16,), float32], framework_op_name="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %311 = transpose(%310, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %312 = nn.relu(%311, framework_op_name="Sequential_60/Conv2d_15/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_15/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %313 = transpose(%312, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %314 = nn.conv2d(%313, meta[relay.Constant][138] // ty=Tensor[(1, 1, 16, 2), float32], framework_op_name="Sequential_60/Conv2d_17/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/Conv2D", channels=2, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 2), float32]
  %315 = add(%314, meta[relay.Constant][139] // ty=Tensor[(2,), float32], framework_op_name="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 2), float32]
  %316 = transpose(%315, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 2, 1200, 1200), float32]
  %317 = transpose(%316, framework_op_name="aten_permute/transpose", output_tensors_name=["aten_permute/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_permute/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 2), float32]
  %318 = (%317, %289)
  %318
}
%319
// meta data omitted. you can use show_meta_data=True to include meta data