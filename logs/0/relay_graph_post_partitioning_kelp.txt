v0.0.1
%329 = fn (%v0:0: Tensor[(1, 3, 2400, 2400), float32]) -> (Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32]) {
  %5 = fn (%v0:01: Tensor[(1, 3, 2400, 2400), float32], __dict__=meta[StrMap][0]) -> Tensor[(1, 3, 2402, 2404), float32] {
    %0 = copy(%v0:01, framework_op_name="copy0", output_tensors_name=["copy0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", axis=0) // ty=Tensor[(1, 3, 2400, 2400), float32]
    %1 = transpose(%0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 3), float32]
    %2 = nn.pad(%1, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 3), float32]
    %3 = transpose(%2, framework_op_name="transpose6", output_tensors_name=["transpose6:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 3, 2402, 2402), float32]
    %4 = nn.pad(%3, framework_op_name="nn.pad0", output_tensors_name=["nn.pad0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", pad_width=[[0, 0], [0, 0], [0, 0], [0, 2]]) // ty=Tensor[(1, 3, 2402, 2404), float32]
    %4
  }
  %6 = %5(%v0:0) // ty=Tensor[(1, 3, 2402, 2404), float32]
  %324 = fn (%nn.pad0:0: Tensor[(1, 3, 2402, 2404), float32], __dict__=meta[StrMap][1]) -> (Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32]) {
    %7 = nn.conv2d(%nn.pad0:0, meta[relay.Constant][0] // ty=Tensor[(3, 3, 3, 64), float32], framework_op_name="nn.conv2d0", output_tensors_name=["nn.conv2d0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], kernel_layout="HWIO") // ty=Tensor[(1, 64, 2400, 2402), float32]
    %8 = strided_slice(%7, framework_op_name="strided_slice0", output_tensors_name=["strided_slice0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", begin=[0, 0, 0, 0], end=[1, 64, 2400, 2400], strides=[1, 1, 1, 1]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %9 = transpose(%8, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %10 = add(%9, meta[relay.Constant][1] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %11 = transpose(%10, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %12 = transpose(%11, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %13 = subtract(%12, meta[relay.Constant][2] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %14 = multiply(%13, meta[relay.Constant][3] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %15 = multiply(%14, meta[relay.Constant][4] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %16 = add(%15, meta[relay.Constant][5] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %17 = transpose(%16, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %18 = nn.relu(%17, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %19 = transpose(%18, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %20 = nn.pad(%19, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 64), float32]
    %21 = nn.conv2d(%20, meta[relay.Constant][6] // ty=Tensor[(3, 3, 64, 64), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 2400, 2400, 64), float32]
    %22 = add(%21, meta[relay.Constant][7] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %23 = transpose(%22, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %24 = transpose(%23, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %25 = subtract(%24, meta[relay.Constant][8] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %26 = multiply(%25, meta[relay.Constant][9] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %27 = multiply(%26, meta[relay.Constant][10] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %28 = add(%27, meta[relay.Constant][11] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %29 = transpose(%28, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %30 = nn.relu(%29, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %31 = transpose(%30, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %32 = nn.max_pool2d(%31, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 1200, 1200, 64), float32]
    %33 = transpose(%32, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %34 = transpose(%33, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %35 = nn.pad(%34, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
    %36 = nn.conv2d(%35, meta[relay.Constant][12] // ty=Tensor[(3, 3, 64, 128), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
    %37 = add(%36, meta[relay.Constant][13] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %38 = transpose(%37, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %39 = transpose(%38, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %40 = subtract(%39, meta[relay.Constant][14] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %41 = multiply(%40, meta[relay.Constant][15] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %42 = multiply(%41, meta[relay.Constant][16] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %43 = add(%42, meta[relay.Constant][17] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %44 = transpose(%43, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %45 = nn.relu(%44, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %46 = transpose(%45, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %47 = nn.pad(%46, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 128), float32]
    %48 = nn.conv2d(%47, meta[relay.Constant][18] // ty=Tensor[(3, 3, 128, 128), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
    %49 = add(%48, meta[relay.Constant][19] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %50 = transpose(%49, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %51 = transpose(%50, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %52 = subtract(%51, meta[relay.Constant][20] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %53 = multiply(%52, meta[relay.Constant][21] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %54 = multiply(%53, meta[relay.Constant][22] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %55 = add(%54, meta[relay.Constant][23] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %56 = transpose(%55, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %57 = nn.relu(%56, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %58 = transpose(%57, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %59 = nn.max_pool2d(%58, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 600, 600, 128), float32]
    %60 = transpose(%59, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %61 = transpose(%60, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %62 = nn.pad(%61, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
    %63 = nn.conv2d(%62, meta[relay.Constant][24] // ty=Tensor[(3, 3, 128, 256), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
    %64 = add(%63, meta[relay.Constant][25] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %65 = transpose(%64, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %66 = transpose(%65, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %67 = subtract(%66, meta[relay.Constant][26] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %68 = multiply(%67, meta[relay.Constant][27] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %69 = multiply(%68, meta[relay.Constant][28] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %70 = add(%69, meta[relay.Constant][29] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %71 = transpose(%70, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %72 = nn.relu(%71, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
    %73 = transpose(%72, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %74 = nn.pad(%73, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
    %75 = nn.conv2d(%74, meta[relay.Constant][30] // ty=Tensor[(3, 3, 256, 256), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
    %76 = add(%75, meta[relay.Constant][31] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %77 = transpose(%76, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %78 = transpose(%77, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %79 = subtract(%78, meta[relay.Constant][32] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %80 = multiply(%79, meta[relay.Constant][33] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %81 = multiply(%80, meta[relay.Constant][34] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %82 = add(%81, meta[relay.Constant][35] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %83 = transpose(%82, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %84 = nn.relu(%83, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
    %85 = transpose(%84, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %86 = nn.pad(%85, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
    %87 = nn.conv2d(%86, meta[relay.Constant][36] // ty=Tensor[(3, 3, 256, 256), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
    %88 = add(%87, meta[relay.Constant][37] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %89 = transpose(%88, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %90 = transpose(%89, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %91 = subtract(%90, meta[relay.Constant][38] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %92 = multiply(%91, meta[relay.Constant][39] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %93 = multiply(%92, meta[relay.Constant][40] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %94 = add(%93, meta[relay.Constant][41] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %95 = transpose(%94, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %96 = nn.relu(%95, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
    %97 = transpose(%96, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %98 = nn.max_pool2d(%97, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 300, 300, 256), float32]
    %99 = transpose(%98, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %100 = transpose(%99, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %101 = nn.pad(%100, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
    %102 = nn.conv2d(%101, meta[relay.Constant][42] // ty=Tensor[(3, 3, 256, 512), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
    %103 = add(%102, meta[relay.Constant][43] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %104 = transpose(%103, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %105 = transpose(%104, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %106 = subtract(%105, meta[relay.Constant][44] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %107 = multiply(%106, meta[relay.Constant][45] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %108 = multiply(%107, meta[relay.Constant][46] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %109 = add(%108, meta[relay.Constant][47] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %110 = transpose(%109, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %111 = nn.relu(%110, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
    %112 = transpose(%111, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %113 = nn.pad(%112, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
    %114 = nn.conv2d(%113, meta[relay.Constant][48] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
    %115 = add(%114, meta[relay.Constant][49] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %116 = transpose(%115, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %117 = transpose(%116, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %118 = subtract(%117, meta[relay.Constant][50] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %119 = multiply(%118, meta[relay.Constant][51] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %120 = multiply(%119, meta[relay.Constant][52] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %121 = add(%120, meta[relay.Constant][53] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %122 = transpose(%121, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %123 = nn.relu(%122, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
    %124 = transpose(%123, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %125 = nn.pad(%124, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
    %126 = nn.conv2d(%125, meta[relay.Constant][54] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
    %127 = add(%126, meta[relay.Constant][55] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %128 = transpose(%127, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %129 = transpose(%128, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %130 = subtract(%129, meta[relay.Constant][56] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %131 = multiply(%130, meta[relay.Constant][57] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %132 = multiply(%131, meta[relay.Constant][58] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %133 = add(%132, meta[relay.Constant][59] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %134 = transpose(%133, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %135 = nn.relu(%134, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
    %136 = transpose(%135, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %137 = nn.max_pool2d(%136, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
    %138 = transpose(%137, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %139 = transpose(%138, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %140 = nn.pad(%139, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %141 = nn.conv2d(%140, meta[relay.Constant][60] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
    %142 = add(%141, meta[relay.Constant][61] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %143 = transpose(%142, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %144 = transpose(%143, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %145 = subtract(%144, meta[relay.Constant][62] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %146 = multiply(%145, meta[relay.Constant][63] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %147 = multiply(%146, meta[relay.Constant][64] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %148 = add(%147, meta[relay.Constant][65] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %149 = transpose(%148, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %150 = nn.relu(%149, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
    %151 = transpose(%150, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %152 = nn.pad(%151, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %153 = nn.conv2d(%152, meta[relay.Constant][66] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
    %154 = add(%153, meta[relay.Constant][67] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %155 = transpose(%154, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %156 = transpose(%155, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %157 = subtract(%156, meta[relay.Constant][68] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %158 = multiply(%157, meta[relay.Constant][69] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %159 = multiply(%158, meta[relay.Constant][70] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %160 = add(%159, meta[relay.Constant][71] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %161 = transpose(%160, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %162 = transpose(%161, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %163 = nn.pad(%162, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", pad_value=-65000, pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %164 = nn.max_pool2d(%163, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", pool_size=[3, 3], pool_type="", layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
    %165 = transpose(%164, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %166 = transpose(%165, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %167 = nn.pad(%166, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", pad_width=[[0, 0], [6, 6], [6, 6], [0, 0]]) // ty=Tensor[(1, 162, 162, 512), float32]
    %168 = nn.conv2d(%167, meta[relay.Constant][72] // ty=Tensor[(3, 3, 512, 1024), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", dilation=[6, 6], channels=1024, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
    %169 = add(%168, meta[relay.Constant][73] // ty=Tensor[(1024,), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
    %170 = transpose(%169, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
    %171 = transpose(%170, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1024), float32]
    %172 = nn.conv2d(%171, meta[relay.Constant][74] // ty=Tensor[(1, 1, 1024, 1024), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", channels=1024, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
    %173 = add(%172, meta[relay.Constant][75] // ty=Tensor[(1024,), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
    %174 = transpose(%173, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
    %175 = (%174, %161)
    %176 = concatenate(%175, framework_op_name="aten_cat/concat", output_tensors_name=["aten_cat/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat/concat", axis=1) // ty=Tensor[(1, 1536, 150, 150), float32]
    %177 = transpose(%176, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1536), float32]
    %178 = nn.conv2d(%177, meta[relay.Constant][76] // ty=Tensor[(1, 1, 1536, 512), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=512, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
    %179 = add(%178, meta[relay.Constant][77] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %180 = transpose(%179, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %181 = transpose(%180, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %182 = subtract(%181, meta[relay.Constant][78] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %183 = multiply(%182, meta[relay.Constant][79] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %184 = multiply(%183, meta[relay.Constant][80] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %185 = add(%184, meta[relay.Constant][81] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %186 = transpose(%185, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %187 = nn.relu(%186, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
    %188 = transpose(%187, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %189 = nn.pad(%188, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %190 = nn.conv2d(%189, meta[relay.Constant][82] // ty=Tensor[(3, 3, 512, 256), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 256), float32]
    %191 = add(%190, meta[relay.Constant][83] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %192 = transpose(%191, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
    %193 = transpose(%192, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
    %194 = subtract(%193, meta[relay.Constant][84] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %195 = multiply(%194, meta[relay.Constant][85] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %196 = multiply(%195, meta[relay.Constant][86] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %197 = add(%196, meta[relay.Constant][87] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %198 = transpose(%197, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
    %199 = nn.relu(%198, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 150, 150), float32]
    %200 = transpose(%199, framework_op_name="aten_upsample_bilinear2d/transpose", output_tensors_name=["aten_upsample_bilinear2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
    %201 = transpose(%200, framework_op_name="transpose0", output_tensors_name=["transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
    %202 = nn.conv2d_transpose(%201, meta[relay.Constant][88] // ty=Tensor[(256, 256, 4, 4), float32], framework_op_name="nn.conv2d_transpose0", output_tensors_name=["nn.conv2d_transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", channels=256, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 256, 300, 300), float32]
    %203 = transpose(%202, framework_op_name="aten_upsample_bilinear2d/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %204 = transpose(%203, framework_op_name="aten_upsample_bilinear2d/transpose_1", output_tensors_name=["aten_upsample_bilinear2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %205 = multiply(%204, meta[relay.Constant][89] // ty=Tensor[(300, 300), float32], framework_op_name="aten_upsample_bilinear2d/Mul", output_tensors_name=["aten_upsample_bilinear2d/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/Mul", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
    %206 = (%205, %123)
    %207 = concatenate(%206, framework_op_name="aten_cat_1/concat", output_tensors_name=["aten_cat_1/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_1/concat", axis=1) // ty=Tensor[(1, 768, 300, 300), float32]
    %208 = transpose(%207, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 768), float32]
    %209 = nn.conv2d(%208, meta[relay.Constant][90] // ty=Tensor[(1, 1, 768, 256), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=256, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 256), float32]
    %210 = add(%209, meta[relay.Constant][91] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %211 = transpose(%210, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %212 = transpose(%211, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %213 = subtract(%212, meta[relay.Constant][92] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %214 = multiply(%213, meta[relay.Constant][93] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %215 = multiply(%214, meta[relay.Constant][94] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %216 = add(%215, meta[relay.Constant][95] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %217 = transpose(%216, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %218 = nn.relu(%217, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
    %219 = transpose(%218, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %220 = nn.pad(%219, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
    %221 = nn.conv2d(%220, meta[relay.Constant][96] // ty=Tensor[(3, 3, 256, 128), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 128), float32]
    %222 = add(%221, meta[relay.Constant][97] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %223 = transpose(%222, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
    %224 = transpose(%223, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
    %225 = subtract(%224, meta[relay.Constant][98] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %226 = multiply(%225, meta[relay.Constant][99] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %227 = multiply(%226, meta[relay.Constant][100] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %228 = add(%227, meta[relay.Constant][101] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %229 = transpose(%228, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
    %230 = nn.relu(%229, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 300, 300), float32]
    %231 = transpose(%230, framework_op_name="aten_upsample_bilinear2d_1/transpose", output_tensors_name=["aten_upsample_bilinear2d_1/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
    %232 = transpose(%231, framework_op_name="transpose2", output_tensors_name=["transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
    %233 = nn.conv2d_transpose(%232, meta[relay.Constant][102] // ty=Tensor[(128, 128, 4, 4), float32], framework_op_name="nn.conv2d_transpose1", output_tensors_name=["nn.conv2d_transpose1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", channels=128, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 128, 600, 600), float32]
    %234 = transpose(%233, framework_op_name="aten_upsample_bilinear2d_1/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_1/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %235 = transpose(%234, framework_op_name="aten_upsample_bilinear2d_1/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_1/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %236 = multiply(%235, meta[relay.Constant][103] // ty=Tensor[(600, 600), float32], framework_op_name="aten_upsample_bilinear2d_1/Mul", output_tensors_name=["aten_upsample_bilinear2d_1/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/Mul", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
    %237 = (%236, %84)
    %238 = concatenate(%237, framework_op_name="aten_cat_2/concat", output_tensors_name=["aten_cat_2/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_2/concat", axis=1) // ty=Tensor[(1, 384, 600, 600), float32]
    %239 = transpose(%238, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 384), float32]
    %240 = nn.conv2d(%239, meta[relay.Constant][104] // ty=Tensor[(1, 1, 384, 128), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=128, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 128), float32]
    %241 = add(%240, meta[relay.Constant][105] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %242 = transpose(%241, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %243 = transpose(%242, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %244 = subtract(%243, meta[relay.Constant][106] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %245 = multiply(%244, meta[relay.Constant][107] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %246 = multiply(%245, meta[relay.Constant][108] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %247 = add(%246, meta[relay.Constant][109] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %248 = transpose(%247, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %249 = nn.relu(%248, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
    %250 = transpose(%249, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %251 = nn.pad(%250, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
    %252 = nn.conv2d(%251, meta[relay.Constant][110] // ty=Tensor[(3, 3, 128, 64), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 64), float32]
    %253 = add(%252, meta[relay.Constant][111] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %254 = transpose(%253, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
    %255 = transpose(%254, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
    %256 = subtract(%255, meta[relay.Constant][112] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %257 = multiply(%256, meta[relay.Constant][113] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %258 = multiply(%257, meta[relay.Constant][114] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %259 = add(%258, meta[relay.Constant][115] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %260 = transpose(%259, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
    %261 = nn.relu(%260, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 600, 600), float32]
    %262 = transpose(%261, framework_op_name="aten_upsample_bilinear2d_2/transpose", output_tensors_name=["aten_upsample_bilinear2d_2/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
    %263 = transpose(%262, framework_op_name="transpose4", output_tensors_name=["transpose4:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
    %264 = nn.conv2d_transpose(%263, meta[relay.Constant][116] // ty=Tensor[(64, 64, 4, 4), float32], framework_op_name="nn.conv2d_transpose2", output_tensors_name=["nn.conv2d_transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", channels=64, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %265 = transpose(%264, framework_op_name="aten_upsample_bilinear2d_2/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_2/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %266 = transpose(%265, framework_op_name="aten_upsample_bilinear2d_2/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_2/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %267 = multiply(%266, meta[relay.Constant][117] // ty=Tensor[(1200, 1200), float32], framework_op_name="aten_upsample_bilinear2d_2/Mul", output_tensors_name=["aten_upsample_bilinear2d_2/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/Mul", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %268 = (%267, %57)
    %269 = concatenate(%268, framework_op_name="aten_cat_3/concat", output_tensors_name=["aten_cat_3/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_3/concat", axis=1) // ty=Tensor[(1, 192, 1200, 1200), float32]
    %270 = transpose(%269, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 192), float32]
    %271 = nn.conv2d(%270, meta[relay.Constant][118] // ty=Tensor[(1, 1, 192, 64), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=64, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 64), float32]
    %272 = add(%271, meta[relay.Constant][119] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %273 = transpose(%272, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %274 = transpose(%273, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %275 = subtract(%274, meta[relay.Constant][120] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %276 = multiply(%275, meta[relay.Constant][121] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %277 = multiply(%276, meta[relay.Constant][122] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %278 = add(%277, meta[relay.Constant][123] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %279 = transpose(%278, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %280 = nn.relu(%279, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %281 = transpose(%280, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %282 = nn.pad(%281, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
    %283 = nn.conv2d(%282, meta[relay.Constant][124] // ty=Tensor[(3, 3, 64, 32), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
    %284 = add(%283, meta[relay.Constant][125] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %285 = transpose(%284, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %286 = transpose(%285, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %287 = subtract(%286, meta[relay.Constant][126] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %288 = multiply(%287, meta[relay.Constant][127] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %289 = multiply(%288, meta[relay.Constant][128] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %290 = add(%289, meta[relay.Constant][129] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %291 = transpose(%290, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %292 = nn.relu(%291, framework_op_name="nn.relu0", output_tensors_name=["nn.relu0:0"], input_tensors_name=[], framework_op_debug_info="", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %293 = transpose(%292, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %294 = nn.pad(%293, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
    %295 = nn.conv2d(%294, meta[relay.Constant][130] // ty=Tensor[(3, 3, 32, 32), float32], framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
    %296 = add(%295, meta[relay.Constant][131] // ty=Tensor[(32,), float32], framework_op_name="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %297 = transpose(%296, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %298 = nn.relu(%297, framework_op_name="Sequential_60/Conv2d_9/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_9/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %299 = transpose(%298, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %300 = nn.pad(%299, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
    %301 = nn.conv2d(%300, meta[relay.Constant][132] // ty=Tensor[(3, 3, 32, 32), float32], framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
    %302 = add(%301, meta[relay.Constant][133] // ty=Tensor[(32,), float32], framework_op_name="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %303 = transpose(%302, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %304 = nn.relu(%303, framework_op_name="Sequential_60/Conv2d_11/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_11/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %305 = transpose(%304, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %306 = nn.pad(%305, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
    %307 = nn.conv2d(%306, meta[relay.Constant][134] // ty=Tensor[(3, 3, 32, 16), float32], framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Conv2D", channels=16, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
    %308 = add(%307, meta[relay.Constant][135] // ty=Tensor[(16,), float32], framework_op_name="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %309 = transpose(%308, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %310 = nn.relu(%309, framework_op_name="Sequential_60/Conv2d_13/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %311 = transpose(%310, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %312 = nn.conv2d(%311, meta[relay.Constant][136] // ty=Tensor[(1, 1, 16, 16), float32], framework_op_name="Sequential_60/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/Conv2D", channels=16, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
    %313 = add(%312, meta[relay.Constant][137] // ty=Tensor[(16,), float32], framework_op_name="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %314 = transpose(%313, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %315 = nn.relu(%314, framework_op_name="Sequential_60/Conv2d_15/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_15/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %316 = transpose(%315, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %317 = nn.conv2d(%316, meta[relay.Constant][138] // ty=Tensor[(1, 1, 16, 2), float32], framework_op_name="Sequential_60/Conv2d_17/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/Conv2D", channels=2, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 2), float32]
    %318 = add(%317, meta[relay.Constant][139] // ty=Tensor[(2,), float32], framework_op_name="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 2), float32]
    %319 = transpose(%318, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 2, 1200, 1200), float32]
    %320 = transpose(%319, framework_op_name="transpose7", output_tensors_name=["transpose7:0"], input_tensors_name=[], framework_op_debug_info="", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 2), float32]
    %321 = copy(%320, framework_op_name="aten_permute/transpose", output_tensors_name=["aten_permute/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_permute/transpose", axis=0) // ty=Tensor[(1, 1200, 1200, 2), float32]
    %322 = copy(%292, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %323 = (%321, %322)
    %323
  }
  %325 = %324(%6) // ty=(Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32])
  %326 = %325.0
  %327 = %325.1
  %328 = (%326, %327)
  %328
}
%329
// meta data omitted. you can use show_meta_data=True to include meta data