v0.0.1
%334 = fn (%v0:0: Tensor[(1, 3, 2400, 2400), float32]) -> (Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32]) {
  %10 = fn (%v0:01: Tensor[(1, 3, 2400, 2400), float32], __dict__=meta[StrMap][0]) -> Tensor[(1, 3, 2402, 2404), float32] {
    %1 = fn (%p0: Tensor[(1, 3, 2400, 2400), float32], __dict__=meta[StrMap][1]) -> Tensor[(1, 3, 2400, 2400), float32] {
      %0 = copy(%p0, framework_op_name="copy0", output_tensors_name=["copy0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", axis=0) // ty=Tensor[(1, 3, 2400, 2400), float32]
      %0
    }
    %2 = %1(%v0:01) // ty=Tensor[(1, 3, 2400, 2400), float32]
    %8 = fn (%p01: Tensor[(1, 3, 2400, 2400), float32], __dict__=meta[StrMap][2]) -> Tensor[(1, 3, 2402, 2404), float32] {
      %3 = transpose(%p01, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 3), float32]
      %4 = nn.pad(%3, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 3), float32]
      %5 = transpose(%4, framework_op_name="transpose6", output_tensors_name=["transpose6:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 3, 2402, 2402), float32]
      %6 = nn.pad(%5, framework_op_name="nn.pad1", output_tensors_name=["nn.pad1:0"], input_tensors_name=[], framework_op_debug_info="", pad_width=[[0, 0], [0, 0], [0, 0], [0, 2]]) // ty=Tensor[(1, 3, 2402, 2404), float32]
      %7 = copy(%6, framework_op_name="nn.pad0", output_tensors_name=["nn.pad0:0"], input_tensors_name=[], framework_op_debug_info="nn.pad0", axis=0) // ty=Tensor[(1, 3, 2402, 2404), float32]
      %7
    }
    %9 = %8(%2) // ty=Tensor[(1, 3, 2402, 2404), float32]
    %9
  }
  %11 = %10(%v0:0) // ty=Tensor[(1, 3, 2402, 2404), float32]
  %329 = fn (%nn.pad0:0: Tensor[(1, 3, 2402, 2404), float32], __dict__=meta[StrMap][3]) -> (Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32]) {
    %12 = nn.conv2d(%nn.pad0:0, meta[relay.Constant][0] // ty=Tensor[(3, 3, 3, 64), float32], framework_op_name="nn.conv2d0", output_tensors_name=["nn.conv2d0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], kernel_layout="HWIO") // ty=Tensor[(1, 64, 2400, 2402), float32]
    %13 = strided_slice(%12, framework_op_name="strided_slice0", output_tensors_name=["strided_slice0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", begin=[0, 0, 0, 0], end=[1, 64, 2400, 2400], strides=[1, 1, 1, 1]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %14 = transpose(%13, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %15 = add(%14, meta[relay.Constant][1] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %16 = transpose(%15, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %17 = transpose(%16, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %18 = subtract(%17, meta[relay.Constant][2] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %19 = multiply(%18, meta[relay.Constant][3] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %20 = multiply(%19, meta[relay.Constant][4] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %21 = add(%20, meta[relay.Constant][5] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %22 = transpose(%21, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %23 = nn.relu(%22, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %24 = transpose(%23, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %25 = nn.pad(%24, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 64), float32]
    %26 = nn.conv2d(%25, meta[relay.Constant][6] // ty=Tensor[(3, 3, 64, 64), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 2400, 2400, 64), float32]
    %27 = add(%26, meta[relay.Constant][7] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %28 = transpose(%27, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %29 = transpose(%28, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %30 = subtract(%29, meta[relay.Constant][8] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %31 = multiply(%30, meta[relay.Constant][9] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %32 = multiply(%31, meta[relay.Constant][10] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %33 = add(%32, meta[relay.Constant][11] // ty=Tensor[(64,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %34 = transpose(%33, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %35 = nn.relu(%34, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
    %36 = transpose(%35, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
    %37 = nn.max_pool2d(%36, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 1200, 1200, 64), float32]
    %38 = transpose(%37, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %39 = transpose(%38, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %40 = nn.pad(%39, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
    %41 = nn.conv2d(%40, meta[relay.Constant][12] // ty=Tensor[(3, 3, 64, 128), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
    %42 = add(%41, meta[relay.Constant][13] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %43 = transpose(%42, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %44 = transpose(%43, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %45 = subtract(%44, meta[relay.Constant][14] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %46 = multiply(%45, meta[relay.Constant][15] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %47 = multiply(%46, meta[relay.Constant][16] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %48 = add(%47, meta[relay.Constant][17] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %49 = transpose(%48, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %50 = nn.relu(%49, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %51 = transpose(%50, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %52 = nn.pad(%51, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 128), float32]
    %53 = nn.conv2d(%52, meta[relay.Constant][18] // ty=Tensor[(3, 3, 128, 128), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
    %54 = add(%53, meta[relay.Constant][19] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %55 = transpose(%54, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %56 = transpose(%55, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %57 = subtract(%56, meta[relay.Constant][20] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %58 = multiply(%57, meta[relay.Constant][21] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %59 = multiply(%58, meta[relay.Constant][22] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %60 = add(%59, meta[relay.Constant][23] // ty=Tensor[(128,), float32], framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %61 = transpose(%60, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %62 = nn.relu(%61, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
    %63 = transpose(%62, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
    %64 = nn.max_pool2d(%63, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 600, 600, 128), float32]
    %65 = transpose(%64, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %66 = transpose(%65, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %67 = nn.pad(%66, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
    %68 = nn.conv2d(%67, meta[relay.Constant][24] // ty=Tensor[(3, 3, 128, 256), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
    %69 = add(%68, meta[relay.Constant][25] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %70 = transpose(%69, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %71 = transpose(%70, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %72 = subtract(%71, meta[relay.Constant][26] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %73 = multiply(%72, meta[relay.Constant][27] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %74 = multiply(%73, meta[relay.Constant][28] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %75 = add(%74, meta[relay.Constant][29] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %76 = transpose(%75, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %77 = nn.relu(%76, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
    %78 = transpose(%77, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %79 = nn.pad(%78, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
    %80 = nn.conv2d(%79, meta[relay.Constant][30] // ty=Tensor[(3, 3, 256, 256), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
    %81 = add(%80, meta[relay.Constant][31] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %82 = transpose(%81, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %83 = transpose(%82, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %84 = subtract(%83, meta[relay.Constant][32] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %85 = multiply(%84, meta[relay.Constant][33] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %86 = multiply(%85, meta[relay.Constant][34] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %87 = add(%86, meta[relay.Constant][35] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %88 = transpose(%87, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %89 = nn.relu(%88, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
    %90 = transpose(%89, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %91 = nn.pad(%90, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
    %92 = nn.conv2d(%91, meta[relay.Constant][36] // ty=Tensor[(3, 3, 256, 256), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
    %93 = add(%92, meta[relay.Constant][37] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %94 = transpose(%93, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %95 = transpose(%94, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %96 = subtract(%95, meta[relay.Constant][38] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %97 = multiply(%96, meta[relay.Constant][39] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %98 = multiply(%97, meta[relay.Constant][40] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %99 = add(%98, meta[relay.Constant][41] // ty=Tensor[(256,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
    %100 = transpose(%99, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
    %101 = nn.relu(%100, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
    %102 = transpose(%101, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
    %103 = nn.max_pool2d(%102, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 300, 300, 256), float32]
    %104 = transpose(%103, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %105 = transpose(%104, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %106 = nn.pad(%105, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
    %107 = nn.conv2d(%106, meta[relay.Constant][42] // ty=Tensor[(3, 3, 256, 512), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
    %108 = add(%107, meta[relay.Constant][43] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %109 = transpose(%108, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %110 = transpose(%109, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %111 = subtract(%110, meta[relay.Constant][44] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %112 = multiply(%111, meta[relay.Constant][45] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %113 = multiply(%112, meta[relay.Constant][46] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %114 = add(%113, meta[relay.Constant][47] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %115 = transpose(%114, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %116 = nn.relu(%115, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
    %117 = transpose(%116, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %118 = nn.pad(%117, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
    %119 = nn.conv2d(%118, meta[relay.Constant][48] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
    %120 = add(%119, meta[relay.Constant][49] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %121 = transpose(%120, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %122 = transpose(%121, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %123 = subtract(%122, meta[relay.Constant][50] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %124 = multiply(%123, meta[relay.Constant][51] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %125 = multiply(%124, meta[relay.Constant][52] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %126 = add(%125, meta[relay.Constant][53] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %127 = transpose(%126, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %128 = nn.relu(%127, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
    %129 = transpose(%128, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %130 = nn.pad(%129, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
    %131 = nn.conv2d(%130, meta[relay.Constant][54] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
    %132 = add(%131, meta[relay.Constant][55] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %133 = transpose(%132, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %134 = transpose(%133, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %135 = subtract(%134, meta[relay.Constant][56] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %136 = multiply(%135, meta[relay.Constant][57] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %137 = multiply(%136, meta[relay.Constant][58] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %138 = add(%137, meta[relay.Constant][59] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
    %139 = transpose(%138, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
    %140 = nn.relu(%139, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
    %141 = transpose(%140, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
    %142 = nn.max_pool2d(%141, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
    %143 = transpose(%142, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %144 = transpose(%143, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %145 = nn.pad(%144, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %146 = nn.conv2d(%145, meta[relay.Constant][60] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
    %147 = add(%146, meta[relay.Constant][61] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %148 = transpose(%147, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %149 = transpose(%148, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %150 = subtract(%149, meta[relay.Constant][62] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %151 = multiply(%150, meta[relay.Constant][63] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %152 = multiply(%151, meta[relay.Constant][64] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %153 = add(%152, meta[relay.Constant][65] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %154 = transpose(%153, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %155 = nn.relu(%154, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
    %156 = transpose(%155, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %157 = nn.pad(%156, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %158 = nn.conv2d(%157, meta[relay.Constant][66] // ty=Tensor[(3, 3, 512, 512), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
    %159 = add(%158, meta[relay.Constant][67] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %160 = transpose(%159, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %161 = transpose(%160, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %162 = subtract(%161, meta[relay.Constant][68] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %163 = multiply(%162, meta[relay.Constant][69] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %164 = multiply(%163, meta[relay.Constant][70] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %165 = add(%164, meta[relay.Constant][71] // ty=Tensor[(512,), float32], framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %166 = transpose(%165, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %167 = transpose(%166, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %168 = nn.pad(%167, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", pad_value=-65000, pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %169 = nn.max_pool2d(%168, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", pool_size=[3, 3], pool_type="", layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
    %170 = transpose(%169, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %171 = transpose(%170, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %172 = nn.pad(%171, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", pad_width=[[0, 0], [6, 6], [6, 6], [0, 0]]) // ty=Tensor[(1, 162, 162, 512), float32]
    %173 = nn.conv2d(%172, meta[relay.Constant][72] // ty=Tensor[(3, 3, 512, 1024), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", dilation=[6, 6], channels=1024, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
    %174 = add(%173, meta[relay.Constant][73] // ty=Tensor[(1024,), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
    %175 = transpose(%174, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
    %176 = transpose(%175, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1024), float32]
    %177 = nn.conv2d(%176, meta[relay.Constant][74] // ty=Tensor[(1, 1, 1024, 1024), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", channels=1024, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
    %178 = add(%177, meta[relay.Constant][75] // ty=Tensor[(1024,), float32], framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
    %179 = transpose(%178, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
    %180 = (%179, %166)
    %181 = concatenate(%180, framework_op_name="aten_cat/concat", output_tensors_name=["aten_cat/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat/concat", axis=1) // ty=Tensor[(1, 1536, 150, 150), float32]
    %182 = transpose(%181, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1536), float32]
    %183 = nn.conv2d(%182, meta[relay.Constant][76] // ty=Tensor[(1, 1, 1536, 512), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=512, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
    %184 = add(%183, meta[relay.Constant][77] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %185 = transpose(%184, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %186 = transpose(%185, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %187 = subtract(%186, meta[relay.Constant][78] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %188 = multiply(%187, meta[relay.Constant][79] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %189 = multiply(%188, meta[relay.Constant][80] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %190 = add(%189, meta[relay.Constant][81] // ty=Tensor[(512,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
    %191 = transpose(%190, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
    %192 = nn.relu(%191, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
    %193 = transpose(%192, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
    %194 = nn.pad(%193, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
    %195 = nn.conv2d(%194, meta[relay.Constant][82] // ty=Tensor[(3, 3, 512, 256), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 256), float32]
    %196 = add(%195, meta[relay.Constant][83] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %197 = transpose(%196, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
    %198 = transpose(%197, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
    %199 = subtract(%198, meta[relay.Constant][84] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %200 = multiply(%199, meta[relay.Constant][85] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %201 = multiply(%200, meta[relay.Constant][86] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %202 = add(%201, meta[relay.Constant][87] // ty=Tensor[(256,), float32], framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
    %203 = transpose(%202, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
    %204 = nn.relu(%203, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 150, 150), float32]
    %205 = transpose(%204, framework_op_name="aten_upsample_bilinear2d/transpose", output_tensors_name=["aten_upsample_bilinear2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
    %206 = transpose(%205, framework_op_name="transpose0", output_tensors_name=["transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
    %207 = nn.conv2d_transpose(%206, meta[relay.Constant][88] // ty=Tensor[(256, 256, 4, 4), float32], framework_op_name="nn.conv2d_transpose0", output_tensors_name=["nn.conv2d_transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", channels=256, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 256, 300, 300), float32]
    %208 = transpose(%207, framework_op_name="aten_upsample_bilinear2d/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %209 = transpose(%208, framework_op_name="aten_upsample_bilinear2d/transpose_1", output_tensors_name=["aten_upsample_bilinear2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %210 = multiply(%209, meta[relay.Constant][89] // ty=Tensor[(300, 300), float32], framework_op_name="aten_upsample_bilinear2d/Mul", output_tensors_name=["aten_upsample_bilinear2d/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/Mul", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
    %211 = (%210, %128)
    %212 = concatenate(%211, framework_op_name="aten_cat_1/concat", output_tensors_name=["aten_cat_1/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_1/concat", axis=1) // ty=Tensor[(1, 768, 300, 300), float32]
    %213 = transpose(%212, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 768), float32]
    %214 = nn.conv2d(%213, meta[relay.Constant][90] // ty=Tensor[(1, 1, 768, 256), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=256, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 256), float32]
    %215 = add(%214, meta[relay.Constant][91] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %216 = transpose(%215, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %217 = transpose(%216, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %218 = subtract(%217, meta[relay.Constant][92] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %219 = multiply(%218, meta[relay.Constant][93] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %220 = multiply(%219, meta[relay.Constant][94] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %221 = add(%220, meta[relay.Constant][95] // ty=Tensor[(256,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
    %222 = transpose(%221, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
    %223 = nn.relu(%222, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
    %224 = transpose(%223, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
    %225 = nn.pad(%224, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
    %226 = nn.conv2d(%225, meta[relay.Constant][96] // ty=Tensor[(3, 3, 256, 128), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 128), float32]
    %227 = add(%226, meta[relay.Constant][97] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %228 = transpose(%227, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
    %229 = transpose(%228, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
    %230 = subtract(%229, meta[relay.Constant][98] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %231 = multiply(%230, meta[relay.Constant][99] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %232 = multiply(%231, meta[relay.Constant][100] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %233 = add(%232, meta[relay.Constant][101] // ty=Tensor[(128,), float32], framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
    %234 = transpose(%233, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
    %235 = nn.relu(%234, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 300, 300), float32]
    %236 = transpose(%235, framework_op_name="aten_upsample_bilinear2d_1/transpose", output_tensors_name=["aten_upsample_bilinear2d_1/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
    %237 = transpose(%236, framework_op_name="transpose2", output_tensors_name=["transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
    %238 = nn.conv2d_transpose(%237, meta[relay.Constant][102] // ty=Tensor[(128, 128, 4, 4), float32], framework_op_name="nn.conv2d_transpose1", output_tensors_name=["nn.conv2d_transpose1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", channels=128, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 128, 600, 600), float32]
    %239 = transpose(%238, framework_op_name="aten_upsample_bilinear2d_1/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_1/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %240 = transpose(%239, framework_op_name="aten_upsample_bilinear2d_1/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_1/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %241 = multiply(%240, meta[relay.Constant][103] // ty=Tensor[(600, 600), float32], framework_op_name="aten_upsample_bilinear2d_1/Mul", output_tensors_name=["aten_upsample_bilinear2d_1/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/Mul", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
    %242 = (%241, %89)
    %243 = concatenate(%242, framework_op_name="aten_cat_2/concat", output_tensors_name=["aten_cat_2/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_2/concat", axis=1) // ty=Tensor[(1, 384, 600, 600), float32]
    %244 = transpose(%243, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 384), float32]
    %245 = nn.conv2d(%244, meta[relay.Constant][104] // ty=Tensor[(1, 1, 384, 128), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=128, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 128), float32]
    %246 = add(%245, meta[relay.Constant][105] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %247 = transpose(%246, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %248 = transpose(%247, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %249 = subtract(%248, meta[relay.Constant][106] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %250 = multiply(%249, meta[relay.Constant][107] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %251 = multiply(%250, meta[relay.Constant][108] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %252 = add(%251, meta[relay.Constant][109] // ty=Tensor[(128,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
    %253 = transpose(%252, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
    %254 = nn.relu(%253, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
    %255 = transpose(%254, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
    %256 = nn.pad(%255, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
    %257 = nn.conv2d(%256, meta[relay.Constant][110] // ty=Tensor[(3, 3, 128, 64), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 64), float32]
    %258 = add(%257, meta[relay.Constant][111] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %259 = transpose(%258, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
    %260 = transpose(%259, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
    %261 = subtract(%260, meta[relay.Constant][112] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %262 = multiply(%261, meta[relay.Constant][113] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %263 = multiply(%262, meta[relay.Constant][114] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %264 = add(%263, meta[relay.Constant][115] // ty=Tensor[(64,), float32], framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
    %265 = transpose(%264, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
    %266 = nn.relu(%265, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 600, 600), float32]
    %267 = transpose(%266, framework_op_name="aten_upsample_bilinear2d_2/transpose", output_tensors_name=["aten_upsample_bilinear2d_2/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
    %268 = transpose(%267, framework_op_name="transpose4", output_tensors_name=["transpose4:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
    %269 = nn.conv2d_transpose(%268, meta[relay.Constant][116] // ty=Tensor[(64, 64, 4, 4), float32], framework_op_name="nn.conv2d_transpose2", output_tensors_name=["nn.conv2d_transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", channels=64, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %270 = transpose(%269, framework_op_name="aten_upsample_bilinear2d_2/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_2/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %271 = transpose(%270, framework_op_name="aten_upsample_bilinear2d_2/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_2/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %272 = multiply(%271, meta[relay.Constant][117] // ty=Tensor[(1200, 1200), float32], framework_op_name="aten_upsample_bilinear2d_2/Mul", output_tensors_name=["aten_upsample_bilinear2d_2/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/Mul", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %273 = (%272, %62)
    %274 = concatenate(%273, framework_op_name="aten_cat_3/concat", output_tensors_name=["aten_cat_3/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_3/concat", axis=1) // ty=Tensor[(1, 192, 1200, 1200), float32]
    %275 = transpose(%274, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 192), float32]
    %276 = nn.conv2d(%275, meta[relay.Constant][118] // ty=Tensor[(1, 1, 192, 64), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=64, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 64), float32]
    %277 = add(%276, meta[relay.Constant][119] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %278 = transpose(%277, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %279 = transpose(%278, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %280 = subtract(%279, meta[relay.Constant][120] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %281 = multiply(%280, meta[relay.Constant][121] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %282 = multiply(%281, meta[relay.Constant][122] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %283 = add(%282, meta[relay.Constant][123] // ty=Tensor[(64,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %284 = transpose(%283, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %285 = nn.relu(%284, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
    %286 = transpose(%285, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
    %287 = nn.pad(%286, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
    %288 = nn.conv2d(%287, meta[relay.Constant][124] // ty=Tensor[(3, 3, 64, 32), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
    %289 = add(%288, meta[relay.Constant][125] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %290 = transpose(%289, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %291 = transpose(%290, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %292 = subtract(%291, meta[relay.Constant][126] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %293 = multiply(%292, meta[relay.Constant][127] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %294 = multiply(%293, meta[relay.Constant][128] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %295 = add(%294, meta[relay.Constant][129] // ty=Tensor[(32,), float32], framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %296 = transpose(%295, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %297 = nn.relu(%296, framework_op_name="nn.relu0", output_tensors_name=["nn.relu0:0"], input_tensors_name=[], framework_op_debug_info="", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %298 = transpose(%297, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %299 = nn.pad(%298, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
    %300 = nn.conv2d(%299, meta[relay.Constant][130] // ty=Tensor[(3, 3, 32, 32), float32], framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
    %301 = add(%300, meta[relay.Constant][131] // ty=Tensor[(32,), float32], framework_op_name="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %302 = transpose(%301, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %303 = nn.relu(%302, framework_op_name="Sequential_60/Conv2d_9/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_9/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %304 = transpose(%303, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %305 = nn.pad(%304, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
    %306 = nn.conv2d(%305, meta[relay.Constant][132] // ty=Tensor[(3, 3, 32, 32), float32], framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
    %307 = add(%306, meta[relay.Constant][133] // ty=Tensor[(32,), float32], framework_op_name="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %308 = transpose(%307, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %309 = nn.relu(%308, framework_op_name="Sequential_60/Conv2d_11/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_11/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %310 = transpose(%309, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
    %311 = nn.pad(%310, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
    %312 = nn.conv2d(%311, meta[relay.Constant][134] // ty=Tensor[(3, 3, 32, 16), float32], framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Conv2D", channels=16, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
    %313 = add(%312, meta[relay.Constant][135] // ty=Tensor[(16,), float32], framework_op_name="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %314 = transpose(%313, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %315 = nn.relu(%314, framework_op_name="Sequential_60/Conv2d_13/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %316 = transpose(%315, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %317 = nn.conv2d(%316, meta[relay.Constant][136] // ty=Tensor[(1, 1, 16, 16), float32], framework_op_name="Sequential_60/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/Conv2D", channels=16, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
    %318 = add(%317, meta[relay.Constant][137] // ty=Tensor[(16,), float32], framework_op_name="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %319 = transpose(%318, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %320 = nn.relu(%319, framework_op_name="Sequential_60/Conv2d_15/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_15/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
    %321 = transpose(%320, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
    %322 = nn.conv2d(%321, meta[relay.Constant][138] // ty=Tensor[(1, 1, 16, 2), float32], framework_op_name="Sequential_60/Conv2d_17/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/Conv2D", channels=2, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 2), float32]
    %323 = add(%322, meta[relay.Constant][139] // ty=Tensor[(2,), float32], framework_op_name="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 2), float32]
    %324 = transpose(%323, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 2, 1200, 1200), float32]
    %325 = transpose(%324, framework_op_name="transpose7", output_tensors_name=["transpose7:0"], input_tensors_name=[], framework_op_debug_info="", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 2), float32]
    %326 = copy(%325, framework_op_name="aten_permute/transpose", output_tensors_name=["aten_permute/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_permute/transpose", axis=0) // ty=Tensor[(1, 1200, 1200, 2), float32]
    %327 = copy(%297, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
    %328 = (%326, %327)
    %328
  }
  %330 = %329(%11) // ty=(Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32])
  %331 = %330.0
  %332 = %330.1
  %333 = (%331, %332)
  %333
}
%334
// meta data omitted. you can use show_meta_data=True to include meta data