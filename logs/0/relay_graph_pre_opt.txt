v0.0.1
%405 = fn (%v0:0: Tensor[(1, 3, 2400, 2400), float32], %vgg16_bn_6/Sequential_5/Conv2d_12/prim_Constant/Const:0: Tensor[(64, 3, 3, 3), float32], %vgg16_bn_6/Sequential_5/Conv2d_12/prim_Constant_1/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant_2/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant_3/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant_1/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/Conv2d_15/prim_Constant/Const:0: Tensor[(64, 64, 3, 3), float32], %vgg16_bn_6/Sequential_5/Conv2d_15/prim_Constant_1/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant_2/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant_3/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant_1/Const:0: Tensor[(64,), float32], %vgg16_bn_6/Sequential_5/Conv2d_19/prim_Constant/Const:0: Tensor[(128, 64, 3, 3), float32], %vgg16_bn_6/Sequential_5/Conv2d_19/prim_Constant_1/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant_2/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant_3/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant_1/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/Conv2d_22/prim_Constant/Const:0: Tensor[(128, 128, 3, 3), float32], %vgg16_bn_6/Sequential_5/Conv2d_22/prim_Constant_1/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant_2/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant_3/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant_1/Const:0: Tensor[(128,), float32], %vgg16_bn_6/Sequential_6/Conv2d_9/prim_Constant/Const:0: Tensor[(256, 128, 3, 3), float32], %vgg16_bn_6/Sequential_6/Conv2d_9/prim_Constant_1/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant_2/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant_3/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant_1/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/Conv2d_12/prim_Constant/Const:0: Tensor[(256, 256, 3, 3), float32], %vgg16_bn_6/Sequential_6/Conv2d_12/prim_Constant_1/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant_2/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant_3/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant_1/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_8/Conv2d_11/prim_Constant/Const:0: Tensor[(256, 256, 3, 3), float32], %vgg16_bn_6/Sequential_8/Conv2d_11/prim_Constant_1/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant_2/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant_3/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant_1/Const:0: Tensor[(256,), float32], %vgg16_bn_6/Sequential_8/Conv2d_15/prim_Constant/Const:0: Tensor[(512, 256, 3, 3), float32], %vgg16_bn_6/Sequential_8/Conv2d_15/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant_2/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant_3/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/Conv2d_18/prim_Constant/Const:0: Tensor[(512, 512, 3, 3), float32], %vgg16_bn_6/Sequential_8/Conv2d_18/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant_2/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant_3/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/Conv2d_11/prim_Constant/Const:0: Tensor[(512, 512, 3, 3), float32], %vgg16_bn_6/Sequential_10/Conv2d_11/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant_2/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant_3/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/Conv2d_15/prim_Constant/Const:0: Tensor[(512, 512, 3, 3), float32], %vgg16_bn_6/Sequential_10/Conv2d_15/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant_2/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant_3/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/Conv2d_18/prim_Constant/Const:0: Tensor[(512, 512, 3, 3), float32], %vgg16_bn_6/Sequential_10/Conv2d_18/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant_2/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant_3/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add/y:0: Tensor[(1,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant_1/Const:0: Tensor[(512,), float32], %vgg16_bn_6/Sequential_12/Conv2d_4/prim_Constant/Const:0: Tensor[(1024, 512, 3, 3), float32], %vgg16_bn_6/Sequential_12/Conv2d_4/prim_Constant_1/Const:0: Tensor[(1024,), float32], %vgg16_bn_6/Sequential_12/Conv2d_5/prim_Constant/Const:0: Tensor[(1024, 1024, 1, 1), float32], %vgg16_bn_6/Sequential_12/Conv2d_5/prim_Constant_1/Const:0: Tensor[(1024,), float32], %double_conv_11/Sequential_1/Conv2d_6/prim_Constant/Const:0: Tensor[(512, 1536, 1, 1), float32], %double_conv_11/Sequential_1/Conv2d_6/prim_Constant_1/Const:0: Tensor[(512,), float32], %double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0: Tensor[(512,), float32], %double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0: Tensor[(512,), float32], %double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0: Tensor[(512,), float32], %double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0: Tensor[(512,), float32], %double_conv_11/Sequential_1/Conv2d_9/prim_Constant/Const:0: Tensor[(256, 512, 3, 3), float32], %double_conv_11/Sequential_1/Conv2d_9/prim_Constant_1/Const:0: Tensor[(256,), float32], %double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0: Tensor[(256,), float32], %double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0: Tensor[(256,), float32], %double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0: Tensor[(256,), float32], %double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0: Tensor[(256,), float32], %aten_upsample_bilinear2d/Const:0: Tensor[(4, 4, 256, 256), float32], %aten_upsample_bilinear2d/Const_1:0: Tensor[(300, 300), float32], %double_conv_27/Sequential_1/Conv2d_6/prim_Constant/Const:0: Tensor[(256, 768, 1, 1), float32], %double_conv_27/Sequential_1/Conv2d_6/prim_Constant_1/Const:0: Tensor[(256,), float32], %double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0: Tensor[(256,), float32], %double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0: Tensor[(256,), float32], %double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0: Tensor[(256,), float32], %double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0: Tensor[(256,), float32], %double_conv_27/Sequential_1/Conv2d_9/prim_Constant/Const:0: Tensor[(128, 256, 3, 3), float32], %double_conv_27/Sequential_1/Conv2d_9/prim_Constant_1/Const:0: Tensor[(128,), float32], %double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0: Tensor[(128,), float32], %double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0: Tensor[(128,), float32], %double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0: Tensor[(128,), float32], %double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0: Tensor[(128,), float32], %aten_upsample_bilinear2d_1/Const:0: Tensor[(4, 4, 128, 128), float32], %aten_upsample_bilinear2d_1/Const_1:0: Tensor[(600, 600), float32], %double_conv_43/Sequential_1/Conv2d_6/prim_Constant/Const:0: Tensor[(128, 384, 1, 1), float32], %double_conv_43/Sequential_1/Conv2d_6/prim_Constant_1/Const:0: Tensor[(128,), float32], %double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0: Tensor[(128,), float32], %double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0: Tensor[(128,), float32], %double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0: Tensor[(128,), float32], %double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0: Tensor[(128,), float32], %double_conv_43/Sequential_1/Conv2d_9/prim_Constant/Const:0: Tensor[(64, 128, 3, 3), float32], %double_conv_43/Sequential_1/Conv2d_9/prim_Constant_1/Const:0: Tensor[(64,), float32], %double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0: Tensor[(64,), float32], %double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0: Tensor[(64,), float32], %double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0: Tensor[(64,), float32], %double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0: Tensor[(64,), float32], %aten_upsample_bilinear2d_2/Const:0: Tensor[(4, 4, 64, 64), float32], %aten_upsample_bilinear2d_2/Const_1:0: Tensor[(1200, 1200), float32], %double_conv_59/Sequential_1/Conv2d_6/prim_Constant/Const:0: Tensor[(64, 192, 1, 1), float32], %double_conv_59/Sequential_1/Conv2d_6/prim_Constant_1/Const:0: Tensor[(64,), float32], %double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0: Tensor[(64,), float32], %double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0: Tensor[(64,), float32], %double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0: Tensor[(64,), float32], %double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0: Tensor[(64,), float32], %double_conv_59/Sequential_1/Conv2d_9/prim_Constant/Const:0: Tensor[(32, 64, 3, 3), float32], %double_conv_59/Sequential_1/Conv2d_9/prim_Constant_1/Const:0: Tensor[(32,), float32], %double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0: Tensor[(32,), float32], %double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0: Tensor[(32,), float32], %double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0: Tensor[(1,), float32], %double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0: Tensor[(32,), float32], %double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0: Tensor[(32,), float32], %Sequential_60/Conv2d_9/prim_Constant/Const:0: Tensor[(32, 32, 3, 3), float32], %Sequential_60/Conv2d_9/prim_Constant_1/Const:0: Tensor[(32,), float32], %Sequential_60/Conv2d_11/prim_Constant/Const:0: Tensor[(32, 32, 3, 3), float32], %Sequential_60/Conv2d_11/prim_Constant_1/Const:0: Tensor[(32,), float32], %Sequential_60/Conv2d_13/prim_Constant/Const:0: Tensor[(16, 32, 3, 3), float32], %Sequential_60/Conv2d_13/prim_Constant_1/Const:0: Tensor[(16,), float32], %Sequential_60/Conv2d_15/prim_Constant/Const:0: Tensor[(16, 16, 1, 1), float32], %Sequential_60/Conv2d_15/prim_Constant_1/Const:0: Tensor[(16,), float32], %Sequential_60/Conv2d_17/prim_Constant/Const:0: Tensor[(2, 16, 1, 1), float32], %Sequential_60/Conv2d_17/prim_Constant_1/Const:0: Tensor[(2,), float32]) -> (Tensor[(1, 1200, 1200, 2), float32], Tensor[(1, 32, 1200, 1200), float32]) {
  %0 = transpose(%v0:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 3), float32]
  %1 = nn.pad(%0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 3), float32]
  %2 = transpose(%vgg16_bn_6/Sequential_5/Conv2d_12/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 3, 64), float32]
  %3 = nn.conv2d(%1, %2, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 2400, 2400, 64), float32]
  %4 = add(%3, %vgg16_bn_6/Sequential_5/Conv2d_12/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %5 = transpose(%4, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %6 = transpose(%5, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %7 = subtract(%6, %vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %8 = add(%vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add", axis=0) // ty=Tensor[(64,), float32]
  %9 = sqrt(%8, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(64,), float32]
  %10 = rdivide_scalar(%9, framework_op_name="rdivide_scalar0", output_tensors_name=["rdivide_scalar0:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(64,), float32]
  %11 = multiply(%7, %10, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %12 = multiply(%11, %vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %13 = add(%12, %vgg16_bn_6/Sequential_5/BatchNorm2d_13/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %14 = transpose(%13, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %15 = nn.relu(%14, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %16 = transpose(%15, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %17 = nn.pad(%16, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 2402, 2402, 64), float32]
  %18 = transpose(%vgg16_bn_6/Sequential_5/Conv2d_15/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 64, 64), float32]
  %19 = nn.conv2d(%17, %18, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 2400, 2400, 64), float32]
  %20 = add(%19, %vgg16_bn_6/Sequential_5/Conv2d_15/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %21 = transpose(%20, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %22 = transpose(%21, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %23 = subtract(%22, %vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %24 = add(%vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add", axis=0) // ty=Tensor[(64,), float32]
  %25 = sqrt(%24, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(64,), float32]
  %26 = rdivide_scalar(%25, framework_op_name="rdivide_scalar1", output_tensors_name=["rdivide_scalar1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(64,), float32]
  %27 = multiply(%23, %26, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %28 = multiply(%27, %vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %29 = add(%28, %vgg16_bn_6/Sequential_5/BatchNorm2d_16/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %30 = transpose(%29, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %31 = nn.relu(%30, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 2400, 2400), float32]
  %32 = transpose(%31, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 2400, 2400, 64), float32]
  %33 = nn.max_pool2d(%32, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 1200, 1200, 64), float32]
  %34 = transpose(%33, framework_op_name="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/MaxPool2d_18/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %35 = transpose(%34, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %36 = nn.pad(%35, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
  %37 = transpose(%vgg16_bn_6/Sequential_5/Conv2d_19/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 64, 128), float32]
  %38 = nn.conv2d(%36, %37, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
  %39 = add(%38, %vgg16_bn_6/Sequential_5/Conv2d_19/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %40 = transpose(%39, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_19/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %41 = transpose(%40, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %42 = subtract(%41, %vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %43 = add(%vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add", axis=0) // ty=Tensor[(128,), float32]
  %44 = sqrt(%43, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(128,), float32]
  %45 = rdivide_scalar(%44, framework_op_name="rdivide_scalar2", output_tensors_name=["rdivide_scalar2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(128,), float32]
  %46 = multiply(%42, %45, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %47 = multiply(%46, %vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %48 = add(%47, %vgg16_bn_6/Sequential_5/BatchNorm2d_20/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %49 = transpose(%48, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %50 = nn.relu(%49, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_20/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %51 = transpose(%50, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %52 = nn.pad(%51, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 128), float32]
  %53 = transpose(%vgg16_bn_6/Sequential_5/Conv2d_22/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 128, 128), float32]
  %54 = nn.conv2d(%52, %53, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 128), float32]
  %55 = add(%54, %vgg16_bn_6/Sequential_5/Conv2d_22/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %56 = transpose(%55, framework_op_name="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/Conv2d_22/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %57 = transpose(%56, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %58 = subtract(%57, %vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %59 = add(%vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add", axis=0) // ty=Tensor[(128,), float32]
  %60 = sqrt(%59, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(128,), float32]
  %61 = rdivide_scalar(%60, framework_op_name="rdivide_scalar3", output_tensors_name=["rdivide_scalar3:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(128,), float32]
  %62 = multiply(%58, %61, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %63 = multiply(%62, %vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %64 = add(%63, %vgg16_bn_6/Sequential_5/BatchNorm2d_23/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %65 = transpose(%64, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %66 = nn.relu(%65, framework_op_name="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_5/BatchNorm2d_23/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 1200, 1200), float32]
  %67 = transpose(%66, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 128), float32]
  %68 = nn.max_pool2d(%67, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 600, 600, 128), float32]
  %69 = transpose(%68, framework_op_name="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/MaxPool2d_8/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %70 = transpose(%69, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %71 = nn.pad(%70, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
  %72 = transpose(%vgg16_bn_6/Sequential_6/Conv2d_9/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 128, 256), float32]
  %73 = nn.conv2d(%71, %72, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
  %74 = add(%73, %vgg16_bn_6/Sequential_6/Conv2d_9/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %75 = transpose(%74, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %76 = transpose(%75, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %77 = subtract(%76, %vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %78 = add(%vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add", axis=0) // ty=Tensor[(256,), float32]
  %79 = sqrt(%78, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(256,), float32]
  %80 = rdivide_scalar(%79, framework_op_name="rdivide_scalar4", output_tensors_name=["rdivide_scalar4:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(256,), float32]
  %81 = multiply(%77, %80, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %82 = multiply(%81, %vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %83 = add(%82, %vgg16_bn_6/Sequential_6/BatchNorm2d_10/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %84 = transpose(%83, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %85 = nn.relu(%84, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
  %86 = transpose(%85, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %87 = nn.pad(%86, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
  %88 = transpose(%vgg16_bn_6/Sequential_6/Conv2d_12/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 256, 256), float32]
  %89 = nn.conv2d(%87, %88, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
  %90 = add(%89, %vgg16_bn_6/Sequential_6/Conv2d_12/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %91 = transpose(%90, framework_op_name="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/Conv2d_12/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %92 = transpose(%91, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %93 = subtract(%92, %vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %94 = add(%vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add", axis=0) // ty=Tensor[(256,), float32]
  %95 = sqrt(%94, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(256,), float32]
  %96 = rdivide_scalar(%95, framework_op_name="rdivide_scalar5", output_tensors_name=["rdivide_scalar5:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(256,), float32]
  %97 = multiply(%93, %96, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %98 = multiply(%97, %vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %99 = add(%98, %vgg16_bn_6/Sequential_6/BatchNorm2d_13/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %100 = transpose(%99, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %101 = nn.relu(%100, framework_op_name="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_6/BatchNorm2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
  %102 = transpose(%101, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %103 = nn.pad(%102, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 256), float32]
  %104 = transpose(%vgg16_bn_6/Sequential_8/Conv2d_11/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 256, 256), float32]
  %105 = nn.conv2d(%103, %104, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 256), float32]
  %106 = add(%105, %vgg16_bn_6/Sequential_8/Conv2d_11/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %107 = transpose(%106, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %108 = transpose(%107, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %109 = subtract(%108, %vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %110 = add(%vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add", axis=0) // ty=Tensor[(256,), float32]
  %111 = sqrt(%110, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(256,), float32]
  %112 = rdivide_scalar(%111, framework_op_name="rdivide_scalar6", output_tensors_name=["rdivide_scalar6:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(256,), float32]
  %113 = multiply(%109, %112, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %114 = multiply(%113, %vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %115 = add(%114, %vgg16_bn_6/Sequential_8/BatchNorm2d_12/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 256), float32]
  %116 = transpose(%115, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 600, 600), float32]
  %117 = nn.relu(%116, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 600, 600), float32]
  %118 = transpose(%117, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 256), float32]
  %119 = nn.max_pool2d(%118, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 300, 300, 256), float32]
  %120 = transpose(%119, framework_op_name="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %121 = transpose(%120, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %122 = nn.pad(%121, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
  %123 = transpose(%vgg16_bn_6/Sequential_8/Conv2d_15/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 256, 512), float32]
  %124 = nn.conv2d(%122, %123, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
  %125 = add(%124, %vgg16_bn_6/Sequential_8/Conv2d_15/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %126 = transpose(%125, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %127 = transpose(%126, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %128 = subtract(%127, %vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %129 = add(%vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add", axis=0) // ty=Tensor[(512,), float32]
  %130 = sqrt(%129, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(512,), float32]
  %131 = rdivide_scalar(%130, framework_op_name="rdivide_scalar7", output_tensors_name=["rdivide_scalar7:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(512,), float32]
  %132 = multiply(%128, %131, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %133 = multiply(%132, %vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %134 = add(%133, %vgg16_bn_6/Sequential_8/BatchNorm2d_16/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %135 = transpose(%134, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %136 = nn.relu(%135, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
  %137 = transpose(%136, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %138 = nn.pad(%137, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
  %139 = transpose(%vgg16_bn_6/Sequential_8/Conv2d_18/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 512, 512), float32]
  %140 = nn.conv2d(%138, %139, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
  %141 = add(%140, %vgg16_bn_6/Sequential_8/Conv2d_18/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %142 = transpose(%141, framework_op_name="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %143 = transpose(%142, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %144 = subtract(%143, %vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %145 = add(%vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add", axis=0) // ty=Tensor[(512,), float32]
  %146 = sqrt(%145, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(512,), float32]
  %147 = rdivide_scalar(%146, framework_op_name="rdivide_scalar8", output_tensors_name=["rdivide_scalar8:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(512,), float32]
  %148 = multiply(%144, %147, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %149 = multiply(%148, %vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %150 = add(%149, %vgg16_bn_6/Sequential_8/BatchNorm2d_19/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %151 = transpose(%150, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %152 = nn.relu(%151, framework_op_name="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_8/BatchNorm2d_19/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
  %153 = transpose(%152, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %154 = nn.pad(%153, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 512), float32]
  %155 = transpose(%vgg16_bn_6/Sequential_10/Conv2d_11/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 512, 512), float32]
  %156 = nn.conv2d(%154, %155, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 512), float32]
  %157 = add(%156, %vgg16_bn_6/Sequential_10/Conv2d_11/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %158 = transpose(%157, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %159 = transpose(%158, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %160 = subtract(%159, %vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %161 = add(%vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add", axis=0) // ty=Tensor[(512,), float32]
  %162 = sqrt(%161, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(512,), float32]
  %163 = rdivide_scalar(%162, framework_op_name="rdivide_scalar9", output_tensors_name=["rdivide_scalar9:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(512,), float32]
  %164 = multiply(%160, %163, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %165 = multiply(%164, %vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %166 = add(%165, %vgg16_bn_6/Sequential_10/BatchNorm2d_12/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 512), float32]
  %167 = transpose(%166, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 300, 300), float32]
  %168 = nn.relu(%167, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_12/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 300, 300), float32]
  %169 = transpose(%168, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 512), float32]
  %170 = nn.max_pool2d(%169, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/MaxPool2d", pool_size=[2, 2], pool_type="", strides=[2, 2], layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
  %171 = transpose(%170, framework_op_name="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/MaxPool2d_14/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %172 = transpose(%171, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %173 = nn.pad(%172, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %174 = transpose(%vgg16_bn_6/Sequential_10/Conv2d_15/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 512, 512), float32]
  %175 = nn.conv2d(%173, %174, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
  %176 = add(%175, %vgg16_bn_6/Sequential_10/Conv2d_15/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %177 = transpose(%176, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %178 = transpose(%177, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %179 = subtract(%178, %vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %180 = add(%vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add", axis=0) // ty=Tensor[(512,), float32]
  %181 = sqrt(%180, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(512,), float32]
  %182 = rdivide_scalar(%181, framework_op_name="rdivide_scalar10", output_tensors_name=["rdivide_scalar10:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(512,), float32]
  %183 = multiply(%179, %182, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %184 = multiply(%183, %vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %185 = add(%184, %vgg16_bn_6/Sequential_10/BatchNorm2d_16/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %186 = transpose(%185, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %187 = nn.relu(%186, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_16/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
  %188 = transpose(%187, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %189 = nn.pad(%188, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %190 = transpose(%vgg16_bn_6/Sequential_10/Conv2d_18/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 512, 512), float32]
  %191 = nn.conv2d(%189, %190, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/Conv2D", channels=512, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
  %192 = add(%191, %vgg16_bn_6/Sequential_10/Conv2d_18/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %193 = transpose(%192, framework_op_name="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/Conv2d_18/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %194 = transpose(%193, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %195 = subtract(%194, %vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant_2/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %196 = add(%vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant_3/Const:0, %vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add/y:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add", axis=0) // ty=Tensor[(512,), float32]
  %197 = sqrt(%196, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/Sqrt", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(512,), float32]
  %198 = rdivide_scalar(%197, framework_op_name="rdivide_scalar11", output_tensors_name=["rdivide_scalar11:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(512,), float32]
  %199 = multiply(%195, %198, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %200 = multiply(%199, %vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %201 = add(%200, %vgg16_bn_6/Sequential_10/BatchNorm2d_19/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %202 = transpose(%201, framework_op_name="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_10/BatchNorm2d_19/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %203 = transpose(%202, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %204 = nn.pad(%203, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/PadV2", pad_value=-65000, pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %205 = nn.max_pool2d(%204, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/MaxPool2d", pool_size=[3, 3], pool_type="", layout="NHWC") // ty=Tensor[(1, 150, 150, 512), float32]
  %206 = transpose(%205, framework_op_name="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/MaxPool2d_3/aten_max_pool2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %207 = transpose(%206, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %208 = nn.pad(%207, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Pad", pad_width=[[0, 0], [6, 6], [6, 6], [0, 0]]) // ty=Tensor[(1, 162, 162, 512), float32]
  %209 = transpose(%vgg16_bn_6/Sequential_12/Conv2d_4/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 512, 1024), float32]
  %210 = nn.conv2d(%208, %209, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/Conv2D", dilation=[6, 6], channels=1024, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
  %211 = add(%210, %vgg16_bn_6/Sequential_12/Conv2d_4/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
  %212 = transpose(%211, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_4/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
  %213 = transpose(%212, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1024), float32]
  %214 = transpose(%vgg16_bn_6/Sequential_12/Conv2d_5/prim_Constant/Const:0, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_1", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(1, 1, 1024, 1024), float32]
  %215 = nn.conv2d(%213, %214, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/Conv2D", channels=1024, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 1024), float32]
  %216 = add(%215, %vgg16_bn_6/Sequential_12/Conv2d_5/prim_Constant_1/Const:0, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 1024), float32]
  %217 = transpose(%216, framework_op_name="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", output_tensors_name=["vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="vgg16_bn_6/Sequential_12/Conv2d_5/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 1024, 150, 150), float32]
  %218 = (%217, %202)
  %219 = concatenate(%218, framework_op_name="aten_cat/concat", output_tensors_name=["aten_cat/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat/concat", axis=1) // ty=Tensor[(1, 1536, 150, 150), float32]
  %220 = transpose(%219, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 1536), float32]
  %221 = transpose(%double_conv_11/Sequential_1/Conv2d_6/prim_Constant/Const:0, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(1, 1, 1536, 512), float32]
  %222 = nn.conv2d(%220, %221, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=512, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 512), float32]
  %223 = add(%222, %double_conv_11/Sequential_1/Conv2d_6/prim_Constant_1/Const:0, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %224 = transpose(%223, framework_op_name="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %225 = transpose(%224, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %226 = subtract(%225, %double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %227 = add(%double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0, %double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", axis=0) // ty=Tensor[(512,), float32]
  %228 = sqrt(%227, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(512,), float32]
  %229 = rdivide_scalar(%228, framework_op_name="rdivide_scalar12", output_tensors_name=["rdivide_scalar12:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(512,), float32]
  %230 = multiply(%226, %229, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %231 = multiply(%230, %double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %232 = add(%231, %double_conv_11/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 512), float32]
  %233 = transpose(%232, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 512, 150, 150), float32]
  %234 = nn.relu(%233, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 512, 150, 150), float32]
  %235 = transpose(%234, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 512), float32]
  %236 = nn.pad(%235, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 152, 152, 512), float32]
  %237 = transpose(%double_conv_11/Sequential_1/Conv2d_9/prim_Constant/Const:0, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 512, 256), float32]
  %238 = nn.conv2d(%236, %237, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=256, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 150, 150, 256), float32]
  %239 = add(%238, %double_conv_11/Sequential_1/Conv2d_9/prim_Constant_1/Const:0, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %240 = transpose(%239, framework_op_name="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
  %241 = transpose(%240, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
  %242 = subtract(%241, %double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %243 = add(%double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0, %double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", axis=0) // ty=Tensor[(256,), float32]
  %244 = sqrt(%243, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(256,), float32]
  %245 = rdivide_scalar(%244, framework_op_name="rdivide_scalar13", output_tensors_name=["rdivide_scalar13:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(256,), float32]
  %246 = multiply(%242, %245, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %247 = multiply(%246, %double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %248 = add(%247, %double_conv_11/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 150, 150, 256), float32]
  %249 = transpose(%248, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
  %250 = nn.relu(%249, framework_op_name="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_11/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 150, 150), float32]
  %251 = transpose(%250, framework_op_name="aten_upsample_bilinear2d/transpose", output_tensors_name=["aten_upsample_bilinear2d/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 150, 150, 256), float32]
  %252 = transpose(%251, framework_op_name="transpose0", output_tensors_name=["transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 150, 150), float32]
  %253 = transpose(%aten_upsample_bilinear2d/Const:0, framework_op_name="transpose1", output_tensors_name=["transpose1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[3, 2, 0, 1]) // ty=Tensor[(256, 256, 4, 4), float32]
  %254 = nn.conv2d_transpose(%252, %253, framework_op_name="nn.conv2d_transpose0", output_tensors_name=["nn.conv2d_transpose0:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", channels=256, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 256, 300, 300), float32]
  %255 = transpose(%254, framework_op_name="aten_upsample_bilinear2d/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %256 = transpose(%255, framework_op_name="aten_upsample_bilinear2d/transpose_1", output_tensors_name=["aten_upsample_bilinear2d/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %257 = multiply(%256, %aten_upsample_bilinear2d/Const_1:0, framework_op_name="aten_upsample_bilinear2d/Mul", output_tensors_name=["aten_upsample_bilinear2d/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d/Mul", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
  %258 = (%257, %152)
  %259 = concatenate(%258, framework_op_name="aten_cat_1/concat", output_tensors_name=["aten_cat_1/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_1/concat", axis=1) // ty=Tensor[(1, 768, 300, 300), float32]
  %260 = transpose(%259, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 768), float32]
  %261 = transpose(%double_conv_27/Sequential_1/Conv2d_6/prim_Constant/Const:0, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(1, 1, 768, 256), float32]
  %262 = nn.conv2d(%260, %261, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=256, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 256), float32]
  %263 = add(%262, %double_conv_27/Sequential_1/Conv2d_6/prim_Constant_1/Const:0, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %264 = transpose(%263, framework_op_name="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %265 = transpose(%264, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %266 = subtract(%265, %double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %267 = add(%double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0, %double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", axis=0) // ty=Tensor[(256,), float32]
  %268 = sqrt(%267, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(256,), float32]
  %269 = rdivide_scalar(%268, framework_op_name="rdivide_scalar14", output_tensors_name=["rdivide_scalar14:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(256,), float32]
  %270 = multiply(%266, %269, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %271 = multiply(%270, %double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %272 = add(%271, %double_conv_27/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 256), float32]
  %273 = transpose(%272, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 256, 300, 300), float32]
  %274 = nn.relu(%273, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 256, 300, 300), float32]
  %275 = transpose(%274, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 256), float32]
  %276 = nn.pad(%275, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 302, 302, 256), float32]
  %277 = transpose(%double_conv_27/Sequential_1/Conv2d_9/prim_Constant/Const:0, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 256, 128), float32]
  %278 = nn.conv2d(%276, %277, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=128, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 300, 300, 128), float32]
  %279 = add(%278, %double_conv_27/Sequential_1/Conv2d_9/prim_Constant_1/Const:0, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %280 = transpose(%279, framework_op_name="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
  %281 = transpose(%280, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
  %282 = subtract(%281, %double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %283 = add(%double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0, %double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", axis=0) // ty=Tensor[(128,), float32]
  %284 = sqrt(%283, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(128,), float32]
  %285 = rdivide_scalar(%284, framework_op_name="rdivide_scalar15", output_tensors_name=["rdivide_scalar15:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(128,), float32]
  %286 = multiply(%282, %285, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %287 = multiply(%286, %double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %288 = add(%287, %double_conv_27/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 300, 300, 128), float32]
  %289 = transpose(%288, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
  %290 = nn.relu(%289, framework_op_name="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_27/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 300, 300), float32]
  %291 = transpose(%290, framework_op_name="aten_upsample_bilinear2d_1/transpose", output_tensors_name=["aten_upsample_bilinear2d_1/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 300, 300, 128), float32]
  %292 = transpose(%291, framework_op_name="transpose2", output_tensors_name=["transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 300, 300), float32]
  %293 = transpose(%aten_upsample_bilinear2d_1/Const:0, framework_op_name="transpose3", output_tensors_name=["transpose3:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[3, 2, 0, 1]) // ty=Tensor[(128, 128, 4, 4), float32]
  %294 = nn.conv2d_transpose(%292, %293, framework_op_name="nn.conv2d_transpose1", output_tensors_name=["nn.conv2d_transpose1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", channels=128, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 128, 600, 600), float32]
  %295 = transpose(%294, framework_op_name="aten_upsample_bilinear2d_1/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_1/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %296 = transpose(%295, framework_op_name="aten_upsample_bilinear2d_1/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_1/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %297 = multiply(%296, %aten_upsample_bilinear2d_1/Const_1:0, framework_op_name="aten_upsample_bilinear2d_1/Mul", output_tensors_name=["aten_upsample_bilinear2d_1/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_1/Mul", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
  %298 = (%297, %101)
  %299 = concatenate(%298, framework_op_name="aten_cat_2/concat", output_tensors_name=["aten_cat_2/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_2/concat", axis=1) // ty=Tensor[(1, 384, 600, 600), float32]
  %300 = transpose(%299, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 384), float32]
  %301 = transpose(%double_conv_43/Sequential_1/Conv2d_6/prim_Constant/Const:0, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(1, 1, 384, 128), float32]
  %302 = nn.conv2d(%300, %301, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=128, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 128), float32]
  %303 = add(%302, %double_conv_43/Sequential_1/Conv2d_6/prim_Constant_1/Const:0, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %304 = transpose(%303, framework_op_name="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %305 = transpose(%304, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %306 = subtract(%305, %double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %307 = add(%double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0, %double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", axis=0) // ty=Tensor[(128,), float32]
  %308 = sqrt(%307, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(128,), float32]
  %309 = rdivide_scalar(%308, framework_op_name="rdivide_scalar16", output_tensors_name=["rdivide_scalar16:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(128,), float32]
  %310 = multiply(%306, %309, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %311 = multiply(%310, %double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %312 = add(%311, %double_conv_43/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 128), float32]
  %313 = transpose(%312, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 128, 600, 600), float32]
  %314 = nn.relu(%313, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 128, 600, 600), float32]
  %315 = transpose(%314, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 128), float32]
  %316 = nn.pad(%315, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 602, 602, 128), float32]
  %317 = transpose(%double_conv_43/Sequential_1/Conv2d_9/prim_Constant/Const:0, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 128, 64), float32]
  %318 = nn.conv2d(%316, %317, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 600, 600, 64), float32]
  %319 = add(%318, %double_conv_43/Sequential_1/Conv2d_9/prim_Constant_1/Const:0, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %320 = transpose(%319, framework_op_name="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
  %321 = transpose(%320, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
  %322 = subtract(%321, %double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %323 = add(%double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0, %double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", axis=0) // ty=Tensor[(64,), float32]
  %324 = sqrt(%323, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(64,), float32]
  %325 = rdivide_scalar(%324, framework_op_name="rdivide_scalar17", output_tensors_name=["rdivide_scalar17:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(64,), float32]
  %326 = multiply(%322, %325, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %327 = multiply(%326, %double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %328 = add(%327, %double_conv_43/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 600, 600, 64), float32]
  %329 = transpose(%328, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
  %330 = nn.relu(%329, framework_op_name="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_43/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 600, 600), float32]
  %331 = transpose(%330, framework_op_name="aten_upsample_bilinear2d_2/transpose", output_tensors_name=["aten_upsample_bilinear2d_2/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 600, 600, 64), float32]
  %332 = transpose(%331, framework_op_name="transpose4", output_tensors_name=["transpose4:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 600, 600), float32]
  %333 = transpose(%aten_upsample_bilinear2d_2/Const:0, framework_op_name="transpose5", output_tensors_name=["transpose5:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[3, 2, 0, 1]) // ty=Tensor[(64, 64, 4, 4), float32]
  %334 = nn.conv2d_transpose(%332, %333, framework_op_name="nn.conv2d_transpose2", output_tensors_name=["nn.conv2d_transpose2:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", channels=64, kernel_size=[4, 4], strides=[2, 2], padding=[1, 1, 1, 1]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %335 = transpose(%334, framework_op_name="aten_upsample_bilinear2d_2/conv2d_transpose", output_tensors_name=["aten_upsample_bilinear2d_2/conv2d_transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/conv2d_transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %336 = transpose(%335, framework_op_name="aten_upsample_bilinear2d_2/transpose_1", output_tensors_name=["aten_upsample_bilinear2d_2/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %337 = multiply(%336, %aten_upsample_bilinear2d_2/Const_1:0, framework_op_name="aten_upsample_bilinear2d_2/Mul", output_tensors_name=["aten_upsample_bilinear2d_2/Mul:0"], input_tensors_name=[], framework_op_debug_info="aten_upsample_bilinear2d_2/Mul", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %338 = (%337, %66)
  %339 = concatenate(%338, framework_op_name="aten_cat_3/concat", output_tensors_name=["aten_cat_3/concat:0"], input_tensors_name=[], framework_op_debug_info="aten_cat_3/concat", axis=1) // ty=Tensor[(1, 192, 1200, 1200), float32]
  %340 = transpose(%339, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 192), float32]
  %341 = transpose(%double_conv_59/Sequential_1/Conv2d_6/prim_Constant/Const:0, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(1, 1, 192, 64), float32]
  %342 = nn.conv2d(%340, %341, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/Conv2D", channels=64, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 64), float32]
  %343 = add(%342, %double_conv_59/Sequential_1/Conv2d_6/prim_Constant_1/Const:0, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %344 = transpose(%343, framework_op_name="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_6/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %345 = transpose(%344, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %346 = subtract(%345, %double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant_2/Const:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %347 = add(%double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant_3/Const:0, %double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add/y:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add", axis=0) // ty=Tensor[(64,), float32]
  %348 = sqrt(%347, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(64,), float32]
  %349 = rdivide_scalar(%348, framework_op_name="rdivide_scalar18", output_tensors_name=["rdivide_scalar18:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(64,), float32]
  %350 = multiply(%346, %349, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %351 = multiply(%350, %double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant/Const:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %352 = add(%351, %double_conv_59/Sequential_1/BatchNorm2d_7/prim_Constant_1/Const:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %353 = transpose(%352, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %354 = nn.relu(%353, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_7/aten_relu/Relu", axis=0) // ty=Tensor[(1, 64, 1200, 1200), float32]
  %355 = transpose(%354, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 64), float32]
  %356 = nn.pad(%355, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 64), float32]
  %357 = transpose(%double_conv_59/Sequential_1/Conv2d_9/prim_Constant/Const:0, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 64, 32), float32]
  %358 = nn.conv2d(%356, %357, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
  %359 = add(%358, %double_conv_59/Sequential_1/Conv2d_9/prim_Constant_1/Const:0, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %360 = transpose(%359, framework_op_name="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %361 = transpose(%360, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %362 = subtract(%361, %double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant_2/Const:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/sub", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %363 = add(%double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant_3/Const:0, %double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add/y:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add", axis=0) // ty=Tensor[(32,), float32]
  %364 = sqrt(%363, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/Sqrt", axis=0) // ty=Tensor[(32,), float32]
  %365 = rdivide_scalar(%364, framework_op_name="rdivide_scalar19", output_tensors_name=["rdivide_scalar19:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", scalar=1) // ty=Tensor[(32,), float32]
  %366 = multiply(%362, %365, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/truediv", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %367 = multiply(%366, %double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant/Const:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/mul", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %368 = add(%367, %double_conv_59/Sequential_1/BatchNorm2d_10/prim_Constant_1/Const:0, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/add_1", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %369 = transpose(%368, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_batch_norm/transpose_1", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %370 = nn.relu(%369, framework_op_name="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", output_tensors_name=["double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="double_conv_59/Sequential_1/BatchNorm2d_10/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %371 = transpose(%370, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %372 = nn.pad(%371, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
  %373 = transpose(%Sequential_60/Conv2d_9/prim_Constant/Const:0, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose_1", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 32, 32), float32]
  %374 = nn.conv2d(%372, %373, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
  %375 = add(%374, %Sequential_60/Conv2d_9/prim_Constant_1/Const:0, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %376 = transpose(%375, framework_op_name="Sequential_60/Conv2d_9/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_9/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %377 = nn.relu(%376, framework_op_name="Sequential_60/Conv2d_9/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_9/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_9/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %378 = transpose(%377, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %379 = nn.pad(%378, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
  %380 = transpose(%Sequential_60/Conv2d_11/prim_Constant/Const:0, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose_1", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 32, 32), float32]
  %381 = nn.conv2d(%379, %380, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/Conv2D", channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 32), float32]
  %382 = add(%381, %Sequential_60/Conv2d_11/prim_Constant_1/Const:0, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %383 = transpose(%382, framework_op_name="Sequential_60/Conv2d_11/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_11/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %384 = nn.relu(%383, framework_op_name="Sequential_60/Conv2d_11/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_11/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_11/aten_relu/Relu", axis=0) // ty=Tensor[(1, 32, 1200, 1200), float32]
  %385 = transpose(%384, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 32), float32]
  %386 = nn.pad(%385, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Pad", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Pad:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Pad", pad_width=[[0, 0], [1, 1], [1, 1], [0, 0]]) // ty=Tensor[(1, 1202, 1202, 32), float32]
  %387 = transpose(%Sequential_60/Conv2d_13/prim_Constant/Const:0, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose_1", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(3, 3, 32, 16), float32]
  %388 = nn.conv2d(%386, %387, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/Conv2D", channels=16, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
  %389 = add(%388, %Sequential_60/Conv2d_13/prim_Constant_1/Const:0, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %390 = transpose(%389, framework_op_name="Sequential_60/Conv2d_13/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_13/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %391 = nn.relu(%390, framework_op_name="Sequential_60/Conv2d_13/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_13/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_13/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %392 = transpose(%391, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %393 = transpose(%Sequential_60/Conv2d_15/prim_Constant/Const:0, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose_1", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(1, 1, 16, 16), float32]
  %394 = nn.conv2d(%392, %393, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/Conv2D", channels=16, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 16), float32]
  %395 = add(%394, %Sequential_60/Conv2d_15/prim_Constant_1/Const:0, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %396 = transpose(%395, framework_op_name="Sequential_60/Conv2d_15/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_15/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %397 = nn.relu(%396, framework_op_name="Sequential_60/Conv2d_15/aten_relu/Relu", output_tensors_name=["Sequential_60/Conv2d_15/aten_relu/Relu:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_15/aten_relu/Relu", axis=0) // ty=Tensor[(1, 16, 1200, 1200), float32]
  %398 = transpose(%397, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 16), float32]
  %399 = transpose(%Sequential_60/Conv2d_17/prim_Constant/Const:0, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose_1", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose_1:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose_1", axes=[2, 3, 1, 0]) // ty=Tensor[(1, 1, 16, 2), float32]
  %400 = nn.conv2d(%398, %399, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/Conv2D", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/Conv2D:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/Conv2D", channels=2, kernel_size=[1, 1], data_layout="NHWC", kernel_layout="HWIO") // ty=Tensor[(1, 1200, 1200, 2), float32]
  %401 = add(%400, %Sequential_60/Conv2d_17/prim_Constant_1/Const:0, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/BiasAdd:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/BiasAdd", axis=0) // ty=Tensor[(1, 1200, 1200, 2), float32]
  %402 = transpose(%401, framework_op_name="Sequential_60/Conv2d_17/aten__convolution/transpose_2", output_tensors_name=["Sequential_60/Conv2d_17/aten__convolution/transpose_2:0"], input_tensors_name=[], framework_op_debug_info="Sequential_60/Conv2d_17/aten__convolution/transpose_2", axes=[0, 3, 1, 2]) // ty=Tensor[(1, 2, 1200, 1200), float32]
  %403 = transpose(%402, framework_op_name="aten_permute/transpose", output_tensors_name=["aten_permute/transpose:0"], input_tensors_name=[], framework_op_debug_info="aten_permute/transpose", axes=[0, 2, 3, 1]) // ty=Tensor[(1, 1200, 1200, 2), float32]
  %404 = (%403, %370)
  %404
}
%405